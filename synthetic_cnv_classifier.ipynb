{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnYechanJo/Novo-Nordisk_Anomaly-Detection/blob/main/synthetic_cnv_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Synthetic CNV Image Classifier\n",
        "\n",
        "This notebook uses CNV images from the OCT2017 dataset to generate synthetic CNV samples using a Stable Diffusion model. These synthetic images are then mixed with real CNV images at varying ratios (from 0% to 100%) to train a classifier.\n",
        "The goal is to identify the optimal ratio of synthetic data that maximizes classifier performance.\n",
        "\n",
        "**Execution Steps**:\n",
        "1. Data Preprocessing: Convert OCT2017 CNV/NORMAL images into ViT embeddings → Save as `pre-trained_dataset.pt`.\n",
        "2. Diffusion Model Fine-Tuning: Fine-tune the Stable Diffusion UNet on real CNV images.\n",
        "3. Synthetic CNV Image Generation: Use the fine-tuned model to generate synthetic CNV images → Convert to ViT embeddings → Save as  `synthetic_cnv_dataset.pt`.\n",
        "4. Classifier Training by Ratio: Mix real and synthetic CNV data at different ratios (0% to 100%) → Train classifier → Compare performance.\n",
        "\n",
        "\n",
        "**Environment**: Google Colab Google Colab (GPU recommended, e.g., T4 or A100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cd5dba-ce48-446c-d68a-a6f76d4dfb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, xformers, pytorch-fid\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-fid-0.3.0 xformers-0.0.29.post3 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# 메모리 정리 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess"
      },
      "source": [
        "## 2. Data Preprocessing: OCT2017 Dataset → ViT Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "e5b718b514e24523affd909cf689cfe5",
            "8d3a074523684d03ae9b621defe66621",
            "7c91961720804032b99f211a7f3d711b",
            "d3e0e5db42f546c5ac6f59290b1d7af2",
            "5f6c1dcd76e24f5eb209464cf3ba9047",
            "54bbe938fe5d40d0b4342bbd12d28c59",
            "9d0fa4a0c3d3425985e8498c416d6bec",
            "d177656a4c514737a8829a16663300d9",
            "41c84aeb492b4b27acb573377edfc238",
            "607fe033ce224c2384902bc5e3b8e724",
            "e3556a7820964214a107efca7d76ab69",
            "959d890e84c242a896477e31c542be69",
            "c6df661567504ea697522590c8bd767b",
            "ea1a7cc225ff40d0b277066ce478ebee",
            "6b261a12acc241bbb37f6c2ca655c247",
            "8006ce308af94ad58066623c0cdee19f",
            "4cd7e1a117104e5f9e166f2f3aa3bc46",
            "327d445f45dd4098ac6a0e3b882cc5ba",
            "87eede00f6ef4fac89503a6c94b98c6a",
            "5a2b50df88af48d9a9d1be883bea645a",
            "c3ce907a2e8241c7a9b583cd462bc674",
            "8b28066d319a4f0ba033fdeed462818a"
          ]
        },
        "outputId": "13f924ef-caf0-4330-c8fa-4cdc8ca99654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing: Start!\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/kermany2018?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10.8G/10.8G [04:30<00:00, 43.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5b718b514e24523affd909cf689cfe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "959d890e84c242a896477e31c542be69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing: Done!\n"
          ]
        }
      ],
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400))),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def vit_process(img_list, batch_size=16):\n",
        "    model_name = \"google/vit-base-patch16-224\"\n",
        "    vit_model = ViTModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    vit_model.eval()\n",
        "    length = len(img_list)\n",
        "    layers_0, layers_1, layers_2, layers_3 = [], [], [], []\n",
        "    for i in range(int(length/batch_size)):\n",
        "        batch_imgs = img_list[i*batch_size:(i+1)*batch_size]\n",
        "        batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = vit_model(pixel_values=batch_tensor)\n",
        "        hidden = outputs.hidden_states\n",
        "        last_layer = hidden[12][:,0,:]\n",
        "        hidden_layer_1 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_2 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_3 = torch.zeros_like(last_layer)\n",
        "        for j in range(12):\n",
        "            if j < 4:\n",
        "                hidden_layer_1 += hidden[j][:,0,:]/4\n",
        "            elif j < 8:\n",
        "                hidden_layer_2 += hidden[j][:,0,:]/4\n",
        "            else:\n",
        "                hidden_layer_3 += hidden[j][:,0,:]/4\n",
        "        layers_0.append(last_layer)\n",
        "        layers_1.append(hidden_layer_1)\n",
        "        layers_2.append(hidden_layer_2)\n",
        "        layers_3.append(hidden_layer_3)\n",
        "        clear_memory()\n",
        "    return (torch.cat(layers_0, dim=0), torch.cat(layers_1, dim=0),\n",
        "            torch.cat(layers_2, dim=0), torch.cat(layers_3, dim=0))\n",
        "\n",
        "def load_trans(path, pic_num=640):\n",
        "    trans_toTensor = img_transform()\n",
        "    image_list = []\n",
        "    i = 0\n",
        "    for filename in os.listdir(path):\n",
        "        if i == pic_num:\n",
        "            break\n",
        "        file_path = os.path.join(path, filename)\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            try:\n",
        "                img = Image.open(file_path).convert(\"RGB\")\n",
        "                tensor_img = trans_toTensor(img)\n",
        "                image_list.append(tensor_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Skip: {filename}, Error: {e}\")\n",
        "        i += 1\n",
        "    return image_list\n",
        "\n",
        "def preprocess_data():\n",
        "    print(\"Data Preprocessing: Start!\")\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    loadpath = os.path.join(path, \"OCT2017 /train\")\n",
        "    train_path_cnv = os.path.join(loadpath, \"CNV\")\n",
        "    train_path_normal = os.path.join(loadpath, \"NORMAL\")\n",
        "\n",
        "    cnv_tensor_list = load_trans(train_path_cnv)\n",
        "    normal_tensor_list = load_trans(train_path_normal)\n",
        "    out_00, out_01, out_02, out_03 = vit_process(cnv_tensor_list)\n",
        "    out_10, out_11, out_12, out_13 = vit_process(normal_tensor_list)\n",
        "\n",
        "    batch_size, batch_num, half_batch = 128, 10, 64\n",
        "    tensor_list_0, tensor_list_1, tensor_list_2, tensor_list_3, labels_list = [], [], [], [], []\n",
        "    for i in range(batch_num):\n",
        "        cnv_tensor_0 = out_00[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_0 = out_10[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_1 = out_01[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_1 = out_11[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_2 = out_02[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_2 = out_12[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_3 = out_03[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_3 = out_13[i*half_batch:(i+1)*half_batch]\n",
        "        tensor_0 = torch.cat((cnv_tensor_0, norm_tensor_0), dim=0)\n",
        "        tensor_1 = torch.cat((cnv_tensor_1, norm_tensor_1), dim=0)\n",
        "        tensor_2 = torch.cat((cnv_tensor_2, norm_tensor_2), dim=0)\n",
        "        tensor_3 = torch.cat((cnv_tensor_3, norm_tensor_3), dim=0)\n",
        "        labels = torch.cat([torch.zeros(half_batch, dtype=torch.long), torch.ones(half_batch, dtype=torch.long)], dim=0)\n",
        "        indices = torch.randperm(batch_size)\n",
        "        tensor_list_0.append(tensor_0[indices])\n",
        "        tensor_list_1.append(tensor_1[indices])\n",
        "        tensor_list_2.append(tensor_2[indices])\n",
        "        tensor_list_3.append(tensor_3[indices])\n",
        "        labels_list.append(labels[indices])\n",
        "    tensor_set_0 = torch.cat(tensor_list_0, dim=0)\n",
        "    tensor_set_1 = torch.cat(tensor_list_1, dim=0)\n",
        "    tensor_set_2 = torch.cat(tensor_list_2, dim=0)\n",
        "    tensor_set_3 = torch.cat(tensor_list_3, dim=0)\n",
        "    labels_set = torch.cat(labels_list, dim=0)\n",
        "\n",
        "    if os.path.exists('pre-trained_dataset.pt'):\n",
        "        os.remove('pre-trained_dataset.pt')\n",
        "    torch.save({'data_0': tensor_set_0, 'data_1': tensor_set_1, 'data_2': tensor_set_2, 'data_3': tensor_set_3, 'label': labels_set}, 'pre-trained_dataset.pt')\n",
        "    print('Data Preprocessing: Done!')\n",
        "\n",
        "preprocess_data()\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diffusion_model"
      },
      "source": [
        "## 3. Diffusion Model Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "finetune_diffusion",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "38d18662-3552-49da-c87e-fad974c0e686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /kaggle/input/kermany2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7faf3c4d9e40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-66116c41c6fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Diffusion Model Fine-tuning Complete: Saved to {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mprepare_cnv_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mtrain_diffusion_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-66116c41c6fc>\u001b[0m in \u001b[0;36mprepare_cnv_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLANCZOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 )\n\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "def prepare_cnv_images():\n",
        "    # Kaggle 데이터셋 다운로드\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # 데이터셋 경로 설정 (공백 제거 및 실제 구조 반영)\n",
        "    in_dir = os.path.join(path, \"OCT2017 /train/CNV\")\n",
        "    out_dir = \"/content/processed/CNV/\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for fn in os.listdir(in_dir):\n",
        "        img = Image.open(os.path.join(in_dir, fn)).convert(\"RGB\")\n",
        "        img = img.resize((512, 512), resample=Image.LANCZOS)\n",
        "        img.save(os.path.join(out_dir, fn))\n",
        "    print(f\"Processed CNV images saved to {out_dir}\")\n",
        "\n",
        "class CNVDataset(Dataset):\n",
        "    def __init__(self, root_dir, tokenizer, resolution=512, max_length=77):\n",
        "        self.files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "        self.prompt = \"OCT scan showing CNV\"\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(self.prompt, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze(0)}\n",
        "\n",
        "def train_diffusion_model(pretrained_model=\"runwayml/stable-diffusion-v1-5\", data_dir=\"/content/processed/CNV\", output_dir=\"/content/sd_cnv_finetuned\", resolution=512, batch_size=2, learning_rate=1e-4, epochs=5, grad_accum_steps=1, save_steps=1000, resume_checkpoint=None):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    accel = Accelerator()\n",
        "    device = accel.device\n",
        "\n",
        "    if resume_checkpoint and unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(resume_checkpoint).to(device)\n",
        "    else:\n",
        "        unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
        "\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
        "    text_encoder.requires_grad_(False)\n",
        "    vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
        "    vae.requires_grad_(False)\n",
        "    scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
        "\n",
        "    dataset = CNVDataset(data_dir, tokenizer, resolution=resolution)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "    unet, optimizer, dataloader = accel.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    if resume_checkpoint:\n",
        "        accel.load_state(resume_checkpoint)\n",
        "        global_step = int(resume_checkpoint.rsplit(\"_\", 1)[-1])\n",
        "    else:\n",
        "        global_step = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accel.accumulate(unet):\n",
        "                pixels = batch[\"pixel_values\"].to(device)\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * 0.18215\n",
        "                noise = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],), device=device)\n",
        "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "                pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "                accel.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            if global_step % save_steps == 0:\n",
        "                accel.wait_for_everyone()\n",
        "                ckpt_dir = os.path.join(output_dir, f\"checkpoint_{global_step}\")\n",
        "                unet.save_pretrained(ckpt_dir)\n",
        "                if accel.is_main_process:\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "                accel.save_state(ckpt_dir)\n",
        "        print(f\"Epoch {epoch}/{epochs} complete\")\n",
        "\n",
        "    accel.wait_for_everyone()\n",
        "    final_dir = os.path.join(output_dir, \"final_unet\")\n",
        "    unet.save_pretrained(final_dir)\n",
        "    if accel.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Diffusion Model Fine-tuning Complete: Saved to {output_dir}\")\n",
        "\n",
        "prepare_cnv_images()\n",
        "train_diffusion_model()\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synthetic_images"
      },
      "source": [
        "## 4. Synthetic CNV Image Generation and ViT Embedding Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "generate_synthetic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "e2c0eb81-8641-4c4f-fa29-1d82a1fb7053"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device: '/content/synthetic_cnv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-97d5e003ec81>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m             torch.cat(layers_2, dim=0), torch.cat(layers_3, dim=0))\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0msynthetic_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_synthetic_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0msynthetic_data_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynthetic_data_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynthetic_data_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynthetic_data_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_process_synthetic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthetic_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m torch.save({\n",
            "\u001b[0;32m<ipython-input-10-97d5e003ec81>\u001b[0m in \u001b[0;36mgenerate_synthetic_images\u001b[0;34m(num_images, output_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_synthetic_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/synthetic_cnv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     pipe = StableDiffusionPipeline.from_pretrained(\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"runwayml/stable-diffusion-v1-5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/content/synthetic_cnv'"
          ]
        }
      ],
      "source": [
        "def generate_synthetic_images(num_images=640, output_dir=\"/content/synthetic_cnv\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "    finetuned_unet = UNet2DConditionModel.from_pretrained(\n",
        "        \"/content/sd_cnv_finetuned/final_unet\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "    pipe.unet = finetuned_unet\n",
        "    ckpt_tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        \"/content/sd_cnv_finetuned\"\n",
        "    )\n",
        "    pipe.tokenizer = ckpt_tokenizer\n",
        "\n",
        "    images = []\n",
        "    prompt = \"OCT scan showing CNV\"\n",
        "    for i in range(num_images):\n",
        "        out = pipe(prompt, num_inference_steps=50, guidance_scale=7.5)\n",
        "        img = out.images[0]\n",
        "        img.save(os.path.join(output_dir, f\"cnv_synthetic_{i}.png\"))\n",
        "        images.append(img)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Generated {i}/{num_images} images\")\n",
        "    return images\n",
        "\n",
        "def vit_process_synthetic(images, batch_size=8):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    model_name = \"google/vit-base-patch16-224\"\n",
        "    vit_model = ViTModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    vit_model.eval()\n",
        "    layers_0, layers_1, layers_2, layers_3 = [], [], [], []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_imgs = [transform(img).to(device) for img in images[i:i+batch_size]]\n",
        "        batch_tensor = torch.stack(batch_imgs, dim=0)\n",
        "        with torch.no_grad():\n",
        "            outputs = vit_model(pixel_values=batch_tensor)\n",
        "        hidden = outputs.hidden_states\n",
        "        last_layer = hidden[12][:,0,:]\n",
        "        hidden_layer_1 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_2 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_3 = torch.zeros_like(last_layer)\n",
        "        for j in range(12):\n",
        "            if j < 4:\n",
        "                hidden_layer_1 += hidden[j][:,0,:]/4\n",
        "            elif j < 8:\n",
        "                hidden_layer_2 += hidden[j][:,0,:]/4\n",
        "            else:\n",
        "                hidden_layer_3 += hidden[j][:,0,:]/4\n",
        "        layers_0.append(last_layer)\n",
        "        layers_1.append(hidden_layer_1)\n",
        "        layers_2.append(hidden_layer_2)\n",
        "        layers_3.append(hidden_layer_3)\n",
        "        clear_memory()\n",
        "    return (torch.cat(layers_0, dim=0), torch.cat(layers_1, dim=0),\n",
        "            torch.cat(layers_2, dim=0), torch.cat(layers_3, dim=0))\n",
        "\n",
        "synthetic_images = generate_synthetic_images()\n",
        "synthetic_data_0, synthetic_data_1, synthetic_data_2, synthetic_data_3 = vit_process_synthetic(synthetic_images)\n",
        "torch.save({\n",
        "    'data_0': synthetic_data_0,\n",
        "    'data_1': synthetic_data_1,\n",
        "    'data_2': synthetic_data_2,\n",
        "    'data_3': synthetic_data_3,\n",
        "    'label': torch.zeros(640, dtype=torch.long)\n",
        "}, 'synthetic_cnv_dataset.pt')\n",
        "print(\"Synthetic CNV Dataset Saved: synthetic_cnv_dataset.pt\")\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classifier"
      },
      "source": [
        "## 5. Define Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "classifier_code"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import abc\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, input_size, d_k=16, d_v=16, n_heads=8, is_layer_norm=False, attn_dropout=0):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_k if d_k is not None else input_size\n",
        "        self.d_v = d_v if d_v is not None else input_size\n",
        "        self.is_layer_norm = is_layer_norm\n",
        "        if self.is_layer_norm:\n",
        "            self.layer_norm = nn.LayerNorm(normalized_shape=input_size)\n",
        "        self.W_q = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
        "        self.W_k = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
        "        self.W_v = nn.Parameter(torch.Tensor(input_size, n_heads * d_v))\n",
        "        self.W_o = nn.Parameter(torch.Tensor(d_v*n_heads, input_size))\n",
        "        self.linear1 = nn.Linear(input_size, input_size)\n",
        "        self.linear2 = nn.Linear(input_size, input_size)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.__init_weights__()\n",
        "\n",
        "    def __init_weights__(self):\n",
        "        init.xavier_normal_(self.W_q)\n",
        "        init.xavier_normal_(self.W_k)\n",
        "        init.xavier_normal_(self.W_v)\n",
        "        init.xavier_normal_(self.W_o)\n",
        "        init.xavier_normal_(self.linear1.weight)\n",
        "        init.xavier_normal_(self.linear2.weight)\n",
        "\n",
        "    def FFN(self, X):\n",
        "        output = self.linear2(F.relu(self.linear1(X)))\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, episilon=1e-6):\n",
        "        temperature = self.d_k ** 0.5\n",
        "        Q_K = torch.einsum(\"bqd,bkd->bqk\", Q, K) / (temperature + episilon)\n",
        "        Q_K_score = F.softmax(Q_K, dim=-1)\n",
        "        Q_K_score = self.dropout(Q_K_score)\n",
        "        V_att = Q_K_score.bmm(V)\n",
        "        return V_att\n",
        "\n",
        "    def multi_head_attention(self, Q, K, V):\n",
        "        bsz, q_len, _ = Q.size()\n",
        "        bsz, k_len, _ = K.size()\n",
        "        bsz, v_len, _ = V.size()\n",
        "        Q_ = Q.matmul(self.W_q).view(bsz, q_len, self.n_heads, self.d_k)\n",
        "        K_ = K.matmul(self.W_k).view(bsz, k_len, self.n_heads, self.d_k)\n",
        "        V_ = V.matmul(self.W_v).view(bsz, k_len, self.n_heads, self.d_v)\n",
        "        Q_ = Q_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, q_len, self.d_k)\n",
        "        K_ = K_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, k_len, self.d_k)\n",
        "        V_ = V_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, k_len, self.d_v)\n",
        "        V_att = self.scaled_dot_product_attention(Q_, K_, V_)\n",
        "        V_att = V_att.view(bsz, self.n_heads, q_len, self.d_v)\n",
        "        V_att = V_att.permute(0, 2, 1, 3).contiguous().view(bsz, q_len, self.n_heads*self.d_v)\n",
        "        output = self.dropout(V_att.matmul(self.W_o))\n",
        "        return output\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        V_att = self.multi_head_attention(Q, K, V)\n",
        "        if self.is_layer_norm:\n",
        "            X = self.layer_norm(Q + V_att)\n",
        "            output = self.layer_norm(self.FFN(X) + X)\n",
        "        else:\n",
        "            X = Q + V_att\n",
        "            output = self.FFN(X) + X\n",
        "        return output\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim=768, output_dim=300, hidden_dim_1=300, hidden_dim_2=450, attn_drop=0.15):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attn_drop = attn_drop\n",
        "        dataset_dic = torch.load('pre-trained_dataset.pt')\n",
        "        embedding_weights_0 = dataset_dic['data_0']\n",
        "        embedding_weights_1 = dataset_dic['data_1']\n",
        "        embedding_weights_2 = dataset_dic['data_2']\n",
        "        embedding_weights_3 = dataset_dic['data_3']\n",
        "        total = 1280\n",
        "        self.embedding_layer_0 = nn.Embedding(num_embeddings=total, embedding_dim=input_dim, padding_idx=0, _weight=embedding_weights_0)\n",
        "        self.embedding_layer_1 = nn.Embedding(num_embeddings=total, embedding_dim=input_dim, padding_idx=0, _weight=embedding_weights_1)\n",
        "        self.embedding_layer_2 = nn.Embedding(num_embeddings=total, embedding_dim=input_dim, padding_idx=0, _weight=embedding_weights_2)\n",
        "        self.embedding_layer_3 = nn.Embedding(num_embeddings=total, embedding_dim=input_dim, padding_idx=0, _weight=embedding_weights_3)\n",
        "        self.linear_1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.linear_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "        self.linear_3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "        self.dropout = nn.Dropout(attn_drop)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.__init_weights__()\n",
        "\n",
        "    def __init_weights__(self):\n",
        "        init.xavier_normal_(self.linear_1.weight)\n",
        "        init.xavier_normal_(self.linear_2.weight)\n",
        "        init.xavier_normal_(self.linear_3.weight)\n",
        "\n",
        "    def forward(self, layer_id=0, X_id=0):\n",
        "        if torch.is_tensor(X_id):\n",
        "            X_id = X_id.to(device)\n",
        "            if layer_id == 0:\n",
        "                X_ = self.embedding_layer_0(X_id).to(torch.float32)\n",
        "            elif layer_id == 1:\n",
        "                X_ = self.embedding_layer_1(X_id).to(torch.float32)\n",
        "            elif layer_id == 2:\n",
        "                X_ = self.embedding_layer_2(X_id).to(torch.float32)\n",
        "            elif layer_id == 3:\n",
        "                X_ = self.embedding_layer_3(X_id).to(torch.float32)\n",
        "        else:\n",
        "            print(\"Non-standard use of encoderblock!\")\n",
        "        residual = self.relu(self.linear_1(X_))\n",
        "        x_ = self.relu(self.dropout(self.linear_2(residual)))\n",
        "        x_ = self.linear_3(x_) + residual\n",
        "        return x_\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.best_acc = 0\n",
        "        self.init_clip_max_norm = None\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, x_test, y_test):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=8e-5, weight_decay=0)\n",
        "        dataset = TensorDataset(x_train, y_train)\n",
        "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        epochs = 15\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "            self.train()\n",
        "            for i, data in enumerate(dataloader):\n",
        "                total = len(dataloader)\n",
        "                batch_x_id, batch_y = (item.cuda() for item in data)\n",
        "                self.batch_dealer(batch_x_id, batch_y, loss, i, epoch+1, total)\n",
        "            self.batch_evaluate(x_val, y_val)\n",
        "\n",
        "    def batch_dealer(self, x_id, y, loss, i, epoch, total):\n",
        "        self.optimizer.zero_grad()\n",
        "        logit_original = self.forward(x_id, epoch=epoch)\n",
        "        loss_classify = loss(logit_original, y)\n",
        "        loss_classify.backward()\n",
        "        self.optimizer.step()\n",
        "        corrects = (torch.max(logit_original, 1)[1].view(y.size()).data == y.data).sum()\n",
        "        accuracy = 100 * corrects / len(y)\n",
        "        print(f'Batch[{i + 1}/{total}] - loss: {loss_classify.item():.6f}  accuracy: {accuracy:.4f}%({corrects}/{y.size(0)})')\n",
        "\n",
        "    def batch_evaluate(self, x, y):\n",
        "        y_pred = self.predicter(x)\n",
        "        acc = accuracy_score(y, y_pred)\n",
        "        if acc > self.best_acc:\n",
        "            self.best_acc = acc\n",
        "        print(classification_report(y, y_pred, target_names=['NR', 'FR'], digits=5))\n",
        "        print(\"Val set acc:\", acc)\n",
        "        print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "    def predicter(self, x):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        self.eval()\n",
        "        y_pred = []\n",
        "        dataset = TensorDataset(x)\n",
        "        dataloader = DataLoader(dataset, batch_size=16)\n",
        "        for i, data in enumerate(dataloader):\n",
        "            with torch.no_grad():\n",
        "                batch_x_id = data[0].cuda()\n",
        "                logits = self.forward(batch_x_id)\n",
        "                predicted = torch.max(logits, dim=1)[1]\n",
        "                y_pred += predicted.data.cpu().numpy().tolist()\n",
        "        return y_pred\n",
        "\n",
        "class Classifier(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder_block = EncoderBlock()\n",
        "        self.attention = TransformerBlock(input_size=300)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(1200, 300)\n",
        "        self.fc1 = nn.Linear(300, 600)\n",
        "        self.fc2 = nn.Linear(600, 300)\n",
        "        self.fc3 = nn.Linear(in_features=300, out_features=2)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init.xavier_normal_(self.fc.weight)\n",
        "        init.xavier_normal_(self.fc1.weight)\n",
        "        init.xavier_normal_(self.fc2.weight)\n",
        "        init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x_id, epoch=0):\n",
        "        batch_size = x_id.shape[0]\n",
        "        x_id = x_id.cuda()\n",
        "        embedding_0 = self.encoder_block(layer_id=0, X_id=x_id)\n",
        "        embedding_1 = self.encoder_block(layer_id=1, X_id=x_id)\n",
        "        embedding_2 = self.encoder_block(layer_id=2, X_id=x_id)\n",
        "        embedding_3 = self.encoder_block(layer_id=3, X_id=x_id)\n",
        "        embedding = self.relu(self.fc(torch.cat((embedding_0, embedding_1, embedding_2, embedding_3), dim=1)))\n",
        "        enhanced = self.attention(embedding.view(batch_size, 1, 300), embedding.view(batch_size, 1, 300), embedding.view(batch_size, 1, 300))\n",
        "        enhanced = enhanced.squeeze(1)\n",
        "        a1 = self.relu(self.dropout(self.fc1(enhanced)))\n",
        "        a1 = self.relu(self.dropout(self.fc2(a1)))\n",
        "        output = self.fc3(a1)\n",
        "        return output\n",
        "\n",
        "def train_and_test(model, x_train, y_train, x_val, y_val, x_test, y_test):\n",
        "    nn = model\n",
        "    nn.fit(x_train, y_train, x_val, y_val, x_test, y_test)\n",
        "    y_pred = nn.predicter(x_test)\n",
        "    res = classification_report(y_test, y_pred, target_names=['NR', 'FR'], digits=3, output_dict=True)\n",
        "    for k, v in res.items():\n",
        "        print(k, v)\n",
        "    print(f\"result: {res['accuracy']:.4f}\")\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_ratios"
      },
      "source": [
        "## 6. Classifier Training and Result Analysis by Synthetic Data Ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_and_analyze"
      },
      "outputs": [],
      "source": [
        "def train_classifier_with_ratios():\n",
        "    normal_dataset = torch.load('pre-trained_dataset.pt')\n",
        "    synthetic_dataset = torch.load('synthetic_cnv_dataset.pt')\n",
        "\n",
        "    normal_cnv_indices = normal_dataset['label'] == 0\n",
        "    normal_normal_indices = normal_dataset['label'] == 1\n",
        "    normal_cnv_data = {\n",
        "        'data_0': normal_dataset['data_0'][normal_cnv_indices],\n",
        "        'data_1': normal_dataset['data_1'][normal_cnv_indices],\n",
        "        'data_2': normal_dataset['data_2'][normal_cnv_indices],\n",
        "        'data_3': normal_dataset['data_3'][normal_cnv_indices],\n",
        "        'label': normal_dataset['label'][normal_cnv_indices]\n",
        "    }\n",
        "    normal_normal_data = {\n",
        "        'data_0': normal_dataset['data_0'][normal_normal_indices],\n",
        "        'data_1': normal_dataset['data_1'][normal_normal_indices],\n",
        "        'data_2': normal_dataset['data_2'][normal_normal_indices],\n",
        "        'data_3': normal_dataset['data_3'][normal_normal_indices],\n",
        "        'label': normal_dataset['label'][normal_normal_indices]\n",
        "    }\n",
        "    synthetic_cnv_data = synthetic_dataset\n",
        "\n",
        "    ratios = np.arange(0, 1.1, 0.1)\n",
        "    results = []\n",
        "\n",
        "    for ratio in ratios:\n",
        "        print(f\"\\nTraining with Synthetic Ratio: {ratio*100:.0f}%\")\n",
        "        num_synthetic = int(640 * ratio)\n",
        "        num_normal_cnv = 640 - num_synthetic\n",
        "        indices_synthetic = np.random.choice(640, num_synthetic, replace=False)\n",
        "        indices_normal_cnv = np.random.choice(640, num_normal_cnv, replace=False)\n",
        "\n",
        "        mixed_data_0 = torch.cat([\n",
        "            synthetic_cnv_data['data_0'][indices_synthetic],\n",
        "            normal_cnv_data['data_0'][indices_normal_cnv],\n",
        "            normal_normal_data['data_0']\n",
        "        ], dim=0)\n",
        "        mixed_data_1 = torch.cat([\n",
        "            synthetic_cnv_data['data_1'][indices_synthetic],\n",
        "            normal_cnv_data['data_1'][indices_normal_cnv],\n",
        "            normal_normal_data['data_1']\n",
        "        ], dim=0)\n",
        "        mixed_data_2 = torch.cat([\n",
        "            synthetic_cnv_data['data_2'][indices_synthetic],\n",
        "            normal_cnv_data['data_2'][indices_normal_cnv],\n",
        "            normal_normal_data['data_2']\n",
        "        ], dim=0)\n",
        "        mixed_data_3 = torch.cat([\n",
        "            synthetic_cnv_data['data_3'][indices_synthetic],\n",
        "            normal_cnv_data['data_3'][indices_normal_cnv],\n",
        "            normal_normal_data['data_3']\n",
        "        ], dim=0)\n",
        "        mixed_labels = torch.cat([\n",
        "            synthetic_cnv_data['label'][indices_synthetic],\n",
        "            normal_cnv_data['label'][indices_normal_cnv],\n",
        "            normal_normal_data['label']\n",
        "        ], dim=0)\n",
        "\n",
        "        x_train = torch.arange(0, 1024)\n",
        "        x_val = torch.arange(1024, 1152)\n",
        "        x_test = torch.arange(1152, 1280)\n",
        "        y_train = mixed_labels[:1024]\n",
        "        y_val = mixed_labels[1024:1152]\n",
        "        y_test = mixed_labels[1152:1280]\n",
        "\n",
        "        # 임베딩 저장\n",
        "        if os.path.exists('pre-trained_dataset.pt'):\n",
        "            os.remove('pre-trained_dataset.pt')\n",
        "        torch.save({\n",
        "            'data_0': mixed_data_0,\n",
        "            'data_1': mixed_data_1,\n",
        "            'data_2': mixed_data_2,\n",
        "            'data_3': mixed_data_3,\n",
        "            'label': mixed_labels\n",
        "        }, 'pre-trained_dataset.pt')\n",
        "\n",
        "        model = Classifier()\n",
        "        res = train_and_test(model, x_train, y_train, x_val, y_val, x_test, y_test)\n",
        "\n",
        "        results.append({\n",
        "            'ratio': ratio,\n",
        "            'accuracy': res['accuracy'],\n",
        "            'f1_score': res['macro avg']['f1-score'],\n",
        "            'precision': res['macro avg']['precision'],\n",
        "            'recall': res['macro avg']['recall']\n",
        "        })\n",
        "        clear_memory()\n",
        "\n",
        "    print(\"\\nResults Summary:\")\n",
        "    for res in results:\n",
        "        print(f\"Ratio: {res['ratio']*100:.0f}% | Accuracy: {res['accuracy']:.4f} | F1 Score: {res['f1_score']:.4f} | Precision: {res['precision']:.4f} | Recall: {res['recall']:.4f}\")\n",
        "\n",
        "    best_result = max(results, key=lambda x: x['accuracy'])\n",
        "    print(f\"\\nBest Ratio: {best_result['ratio']*100:.0f}%\")\n",
        "    print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {best_result['f1_score']:.4f}\")\n",
        "    print(f\"Precision: {best_result['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_result['recall']:.4f}\")\n",
        "\n",
        "    pd.DataFrame(results).to_csv('classifier_results.csv', index=False)\n",
        "    print(\"Results saved to classifier_results.csv\")\n",
        "\n",
        "train_classifier_with_ratios()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fid_analysis"
      },
      "source": [
        "## 7. Quality Analysis of Synthetic Images (FID Score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fid_score"
      },
      "outputs": [],
      "source": [
        "def calculate_fid():\n",
        "    fid = fid_score.calculate_fid_given_paths(\n",
        "        ['/content/processed/CNV', '/content/synthetic_cnv'],\n",
        "        batch_size=50,\n",
        "        device='cuda',\n",
        "        dims=2048\n",
        "    )\n",
        "    print(f\"FID Score: {fid}\")\n",
        "\n",
        "calculate_fid()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5b718b514e24523affd909cf689cfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d3a074523684d03ae9b621defe66621",
              "IPY_MODEL_7c91961720804032b99f211a7f3d711b",
              "IPY_MODEL_d3e0e5db42f546c5ac6f59290b1d7af2"
            ],
            "layout": "IPY_MODEL_5f6c1dcd76e24f5eb209464cf3ba9047"
          }
        },
        "8d3a074523684d03ae9b621defe66621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54bbe938fe5d40d0b4342bbd12d28c59",
            "placeholder": "​",
            "style": "IPY_MODEL_9d0fa4a0c3d3425985e8498c416d6bec",
            "value": "config.json: 100%"
          }
        },
        "7c91961720804032b99f211a7f3d711b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d177656a4c514737a8829a16663300d9",
            "max": 69665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41c84aeb492b4b27acb573377edfc238",
            "value": 69665
          }
        },
        "d3e0e5db42f546c5ac6f59290b1d7af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_607fe033ce224c2384902bc5e3b8e724",
            "placeholder": "​",
            "style": "IPY_MODEL_e3556a7820964214a107efca7d76ab69",
            "value": " 69.7k/69.7k [00:00&lt;00:00, 7.93MB/s]"
          }
        },
        "5f6c1dcd76e24f5eb209464cf3ba9047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54bbe938fe5d40d0b4342bbd12d28c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d0fa4a0c3d3425985e8498c416d6bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d177656a4c514737a8829a16663300d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c84aeb492b4b27acb573377edfc238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "607fe033ce224c2384902bc5e3b8e724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3556a7820964214a107efca7d76ab69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "959d890e84c242a896477e31c542be69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6df661567504ea697522590c8bd767b",
              "IPY_MODEL_ea1a7cc225ff40d0b277066ce478ebee",
              "IPY_MODEL_6b261a12acc241bbb37f6c2ca655c247"
            ],
            "layout": "IPY_MODEL_8006ce308af94ad58066623c0cdee19f"
          }
        },
        "c6df661567504ea697522590c8bd767b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cd7e1a117104e5f9e166f2f3aa3bc46",
            "placeholder": "​",
            "style": "IPY_MODEL_327d445f45dd4098ac6a0e3b882cc5ba",
            "value": "model.safetensors: 100%"
          }
        },
        "ea1a7cc225ff40d0b277066ce478ebee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87eede00f6ef4fac89503a6c94b98c6a",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a2b50df88af48d9a9d1be883bea645a",
            "value": 346293852
          }
        },
        "6b261a12acc241bbb37f6c2ca655c247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ce907a2e8241c7a9b583cd462bc674",
            "placeholder": "​",
            "style": "IPY_MODEL_8b28066d319a4f0ba033fdeed462818a",
            "value": " 346M/346M [00:01&lt;00:00, 350MB/s]"
          }
        },
        "8006ce308af94ad58066623c0cdee19f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd7e1a117104e5f9e166f2f3aa3bc46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "327d445f45dd4098ac6a0e3b882cc5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87eede00f6ef4fac89503a6c94b98c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2b50df88af48d9a9d1be883bea645a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3ce907a2e8241c7a9b583cd462bc674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b28066d319a4f0ba033fdeed462818a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}