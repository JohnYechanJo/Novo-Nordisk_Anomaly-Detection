{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnYechanJo/Novo-Nordisk_Anomaly-Detection/blob/classifier/classifier_demo_converted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import abc\n",
        "import torch.nn.utils as utils\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.init as init\n",
        "import argparse\n",
        "import pickle\n",
        "import json\n",
        "import json, os, time\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "from time import *\n",
        "from PIL import Image\n",
        "\n",
        "# we can load all the data and divide it into 8:1:1 as train/val/test three sets in sequence\n",
        "# 'data_0''data_1''data_2''data_3' & 'label'\n",
        "\n",
        "# Only when the classifier itself has sufficient performance can we better evaluate its impact,\n",
        "# so we need to find a better classifier structure, although classifier complexity is closely related to data complexity"
      ],
      "metadata": {
        "id": "o17A_XvgOreG"
      },
      "id": "o17A_XvgOreG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "71e17d0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e17d0d",
        "outputId": "bd15ac7d-88a6-4094-8a20-993e5f9ebf05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data/pre-trained_dataset.pt\n",
            "Load successfully!\n"
          ]
        }
      ],
      "source": [
        "file_path = os.getcwd()+\"/sample_data/pre-trained_dataset.pt\"\n",
        "print(file_path)\n",
        "if os.path.exists(file_path):\n",
        "    dataset_dic = torch.load(file_path, map_location=torch.device('cpu'))\n",
        "    print(\"Load successfully!\")\n",
        "else:\n",
        "    print(\"File not exist!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c8805688",
      "metadata": {
        "id": "c8805688"
      },
      "outputs": [],
      "source": [
        "# maybe a config file later\n",
        "total = 1280\n",
        "train = 1024\n",
        "val = 128\n",
        "test = 128\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "dropout_rate = 0.6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "labels = dataset_dic['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deal with self-attention"
      ],
      "metadata": {
        "id": "Y8EvJZTAQE4o"
      },
      "id": "Y8EvJZTAQE4o"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "820c939b",
      "metadata": {
        "id": "820c939b"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, d_k=16, d_v=16, n_heads=8, is_layer_norm = False,attn_dropout=0):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_k if d_k is not None else input_size\n",
        "        self.d_v = d_v if d_v is not None else input_size\n",
        "        self.is_layer_norm = is_layer_norm\n",
        "        if self.is_layer_norm:\n",
        "            self.layer_morm = nn.LayerNorm(normalized_shape=input_size)\n",
        "        self.W_q = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
        "        self.W_k = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
        "        self.W_v = nn.Parameter(torch.Tensor(input_size, n_heads * d_v))\n",
        "        self.W_o = nn.Parameter(torch.Tensor(d_v*n_heads, input_size))\n",
        "        self.linear1 = nn.Linear(input_size, input_size)\n",
        "        self.linear2 = nn.Linear(input_size, input_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.__init_weights__()\n",
        "\n",
        "    def __init_weights__(self):\n",
        "        init.xavier_normal_(self.W_q)\n",
        "        init.xavier_normal_(self.W_k)\n",
        "        init.xavier_normal_(self.W_v)\n",
        "        init.xavier_normal_(self.W_o)\n",
        "\n",
        "        init.xavier_normal_(self.linear1.weight)\n",
        "        init.xavier_normal_(self.linear2.weight)\n",
        "\n",
        "    def FFN(self, X):\n",
        "        output = self.linear2(F.relu(self.linear1(X)))\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, episilon=1e-6):\n",
        "        temperature = self.d_k ** 0.5\n",
        "        Q_K = torch.einsum(\"bqd,bkd->bqk\", Q, K) / (temperature + episilon)\n",
        "        Q_K_score = F.softmax(Q_K, dim=-1)\n",
        "        Q_K_score = self.dropout(Q_K_score)\n",
        "        V_att = Q_K_score.bmm(V)\n",
        "        return V_att\n",
        "\n",
        "    def multi_head_attention(self, Q, K, V):\n",
        "        bsz, q_len, _ = Q.size()\n",
        "        bsz, k_len, _ = K.size()\n",
        "        bsz, v_len, _ = V.size()\n",
        "        #[batchsize,len,inputsize] * [inputsize,n_heads*dim_k]\n",
        "        Q_ = Q.matmul(self.W_q).view(bsz, q_len, self.n_heads, self.d_k)\n",
        "        K_ = K.matmul(self.W_k).view(bsz, k_len, self.n_heads, self.d_k)\n",
        "        V_ = V.matmul(self.W_v).view(bsz, k_len, self.n_heads, self.d_v)\n",
        "\n",
        "        Q_ = Q_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, q_len, self.d_k)\n",
        "        K_ = K_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, k_len, self.d_k)\n",
        "        V_ = V_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, k_len, self.d_v)\n",
        "\n",
        "        V_att = self.scaled_dot_product_attention(Q_, K_, V_)\n",
        "        V_att = V_att.view(bsz, self.n_heads, q_len, self.d_v)\n",
        "        V_att = V_att.permute(0, 2, 1, 3).contiguous().view(bsz, q_len, self.n_heads*self.d_v)\n",
        "\n",
        "        output = self.dropout(V_att.matmul(self.W_o))\n",
        "        #[bsz,q_len,inputsize]\n",
        "        return output\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        V_att = self.multi_head_attention(Q, K, V)\n",
        "\n",
        "        if self.is_layer_norm:\n",
        "            X = self.layer_morm(Q + V_att)\n",
        "            output = self.layer_morm(self.FFN(X) + X)\n",
        "        else:\n",
        "            X = Q + V_att\n",
        "            output = self.FFN(X) + X#残差\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add embedding data into gradient flow and do basic trans-size using residual MLP"
      ],
      "metadata": {
        "id": "kMaRF4xVQNJX"
      },
      "id": "kMaRF4xVQNJX"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b2651cff",
      "metadata": {
        "id": "b2651cff"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim = 768, output_dim = 300,hidden_dim_1 = 300,hidden_dim_2 = 450,attn_drop =0.15):\n",
        "        super(EncoderBlock,self).__init__()\n",
        "        self.attn_drop = attn_drop\n",
        "        #[size,768] shuffled pre batch\n",
        "        embedding_weights_0 = dataset_dic['data_0']\n",
        "        embedding_weights_1 = dataset_dic['data_1']\n",
        "        embedding_weights_2 = dataset_dic['data_2']\n",
        "        embedding_weights_3 = dataset_dic['data_3']\n",
        "        self.embedding_layer_0 = nn.Embedding(num_embeddings = total,embedding_dim=input_dim\n",
        "                                            ,padding_idx= 0\n",
        "                                            ,_weight = embedding_weights_0)\n",
        "        self.embedding_layer_1 = nn.Embedding(num_embeddings = total,embedding_dim=input_dim\n",
        "                                            ,padding_idx= 0\n",
        "                                            ,_weight = embedding_weights_1)\n",
        "        self.embedding_layer_2 = nn.Embedding(num_embeddings = total,embedding_dim=input_dim\n",
        "                                            ,padding_idx= 0\n",
        "                                            ,_weight = embedding_weights_2)\n",
        "        self.embedding_layer_3 = nn.Embedding(num_embeddings = total,embedding_dim=input_dim\n",
        "                                            ,padding_idx= 0\n",
        "                                            ,_weight = embedding_weights_3)\n",
        "        self.linear_1 = nn.Linear(input_dim,hidden_dim_1)\n",
        "        self.linear_2 = nn.Linear(hidden_dim_1,hidden_dim_2)\n",
        "        self.linear_3 = nn.Linear(hidden_dim_2,output_dim)\n",
        "        self.dropout = nn.Dropout(attn_drop)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.__init_weights__()\n",
        "\n",
        "    def __init_weights__(self):\n",
        "        init.xavier_normal_(self.linear_1.weight)\n",
        "        init.xavier_normal_(self.linear_2.weight)\n",
        "        init.xavier_normal_(self.linear_3.weight)\n",
        "\n",
        "    #generate idex in sequence and change it to tensor to vist\n",
        "    def forward(self, layer_id=0, X_id=0):\n",
        "        if torch.is_tensor(X_id):\n",
        "            X_id.to(device)\n",
        "            if(layer_id==0):\n",
        "                X_ = self.embedding_layer_0(X_id).to(torch.float32)\n",
        "            elif(layer_id==1):\n",
        "                X_ = self.embedding_layer_1(X_id).to(torch.float32)\n",
        "            elif(layer_id==2):\n",
        "                X_ = self.embedding_layer_2(X_id).to(torch.float32)\n",
        "            elif(layer_id==3):\n",
        "                X_ = self.embedding_layer_3(X_id).to(torch.float32)\n",
        "        else:\n",
        "            print(\"Non-standard use of encoderblock!\")\n",
        "\n",
        "        #residual connect in MLP\n",
        "        residual = self.relu(self.linear_1(X_))\n",
        "        x_ = self.relu(self.dropout(self.linear_2(residual)))\n",
        "        x_ = self.linear_3(x_)+residual\n",
        "\n",
        "        return x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "63c40ed5",
      "metadata": {
        "id": "63c40ed5"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.best_acc = 0\n",
        "        self.init_clip_max_norm = None\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self,x_train,y_train,x_val,y_val,x_test,y_test):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        # set learning rate\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(),lr=8e-5,weight_decay=0)\n",
        "        dataset = TensorDataset(x_train,y_train)\n",
        "        dataloader = DataLoader(dataset,batch_size,shuffle=False)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        # training epochs\n",
        "        for epoch in range(epochs):\n",
        "            print(\"\\nEpoch \", epoch + 1, \"/\", epochs)\n",
        "            self.train()\n",
        "            # Convert to a batched iterator\n",
        "            for i ,data in enumerate(dataloader):\n",
        "                total = len(dataloader)\n",
        "                # unbind\n",
        "                batch_x_id,batch_y = (item.cuda() for item in data)\n",
        "                self.batch_dealer(batch_x_id,batch_y,loss,i,epoch+1,total)\n",
        "            # validation part\n",
        "            self.batch_evaluate(x_val,y_val)\n",
        "\n",
        "    # deal with  x & y batches in training part\n",
        "    def batch_dealer(self,x_id,y,loss,i,epoch,total):\n",
        "        # clean previous grad\n",
        "        self.optimizer.zero_grad()\n",
        "        logit_original = self.forward(x_id,epoch=epoch)\n",
        "        loss_classify = loss(logit_original,y)\n",
        "        loss_classify.backward()\n",
        "        self.optimizer.step()\n",
        "        corrects = (torch.max(logit_original, 1)[1].view(y.size()).data == y.data).sum()\n",
        "        accuracy = 100 * corrects / len(y)\n",
        "        print(\n",
        "            'Batch[{}/{}] - loss: {:.6f}  accuracy: {:.4f}%({}/{})'.format(i + 1, total\n",
        "                                                                            ,loss_classify.item()\n",
        "                                                                            ,accuracy\n",
        "                                                                            ,corrects\n",
        "                                                                            ,y.size(0)))\n",
        "\n",
        "    def batch_evaluate(self,x,y):\n",
        "        y_pred = self.predicter(x)\n",
        "        acc = accuracy_score(y,y_pred)\n",
        "        if acc>self.best_acc:\n",
        "            self.best_acc = acc\n",
        "        print(classification_report(y, y_pred, target_names=['NR', 'FR'], digits=5))\n",
        "        print(\"Val set acc:\", acc)\n",
        "        print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "\n",
        "    def predicter(self,x):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        self.eval()\n",
        "        y_pred = []\n",
        "        dataset = TensorDataset(x)\n",
        "        dataloader = DataLoader(dataset,batch_size=16)\n",
        "        for i,data in enumerate(dataloader):\n",
        "            with torch.no_grad():\n",
        "                batch_x_id = data[0].cuda()\n",
        "                logits = self.forward(batch_x_id)\n",
        "                predicted = torch.max(logits,dim=1)[1]\n",
        "                y_pred += predicted.data.cpu().numpy().tolist()\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9e48ec6c",
      "metadata": {
        "id": "9e48ec6c"
      },
      "outputs": [],
      "source": [
        "class Classifier(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder_block = EncoderBlock()\n",
        "        self.attention = TransformerBlock(input_size=300)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        # classifier struct\n",
        "        self.fc = nn.Linear(1200,300)\n",
        "        self.fc1 = nn.Linear(300,600)\n",
        "        self.fc2 = nn.Linear(600,300)\n",
        "        self.fc3 = nn.Linear(in_features=300, out_features=2)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init.xavier_normal_(self.fc.weight)\n",
        "        init.xavier_normal_(self.fc1.weight)\n",
        "        init.xavier_normal_(self.fc2.weight)\n",
        "        init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "    # every batch\n",
        "    def forward(self,x_id,epoch=0):\n",
        "        batch_size = x_id.shape[0]\n",
        "        x_id.cuda()\n",
        "        # [bsz,300]\n",
        "        embedding_0=self.encoder_block(layer_id =0,X_id=x_id)\n",
        "        embedding_1=self.encoder_block(layer_id =1,X_id=x_id)\n",
        "        embedding_2=self.encoder_block(layer_id =2,X_id=x_id)\n",
        "        embedding_3=self.encoder_block(layer_id =3,X_id=x_id)\n",
        "        # combining information from various hidden layers\n",
        "        # [bsz,1200] -> [bsz,300]\n",
        "        embedding = self.relu(self.fc(torch.cat( (embedding_0\n",
        "                                                 ,embedding_1\n",
        "                                                 ,embedding_2\n",
        "                                                 ,embedding_3),dim=1)))\n",
        "        # trans to a form suitable to the transformer block [bsz,1,300]\n",
        "        enhanced = self.attention(embedding.view(batch_size, 1, 300),embedding.view(batch_size, 1, 300)\n",
        "                                     ,embedding.view(batch_size, 1, 300))\n",
        "        # [bsz,300]\n",
        "        enhanced = enhanced.squeeze(1)\n",
        "        a1 = self.relu(self.dropout(self.fc1(enhanced)))\n",
        "        a1 = self.relu(self.dropout(self.fc2(a1)))\n",
        "        output = self.fc3(a1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e8c2e5ab",
      "metadata": {
        "id": "e8c2e5ab"
      },
      "outputs": [],
      "source": [
        "def train_and_test(model):\n",
        "    # use this as a substitute(index)\n",
        "    x_train = torch.arange(0,1024)\n",
        "    x_val = torch.arange(1024,1152)\n",
        "    x_test = torch.arange(1152,1280)\n",
        "    y_train = labels[0:1024]\n",
        "    y_val = labels[1024:1152]\n",
        "    y_test = labels[1152:1280]\n",
        "    nn=model\n",
        "    #train and val\n",
        "    nn.fit(x_train,y_train,x_val,y_val,x_test,y_test)\n",
        "    #test part\n",
        "    y_pred = nn.predicter(x_test)\n",
        "    res = classification_report(y_test, y_pred, target_names=['NR', 'FR'], digits=3, output_dict=True)\n",
        "    for k, v in res.items():\n",
        "        print(k, v)\n",
        "    print(\"result:{:.4f}\".format(res['accuracy']))\n",
        "    print(res)\n",
        "    # end\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f3198d1c",
      "metadata": {
        "id": "f3198d1c"
      },
      "outputs": [],
      "source": [
        "seed = 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6f8ca6c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8ca6c8",
        "outputId": "17925000-b420-4fe5-cc5b-0d534a2f6f43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79c9dc4f6bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e3b0f3b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3b0f3b9",
        "outputId": "2ecd1360-f8d4-45fe-a92e-641e33c3526a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch  1 / 15\n",
            "Batch[1/8] - loss: 3.958504  accuracy: 54.6875%(70/128)\n",
            "Batch[2/8] - loss: 3.739928  accuracy: 45.3125%(58/128)\n",
            "Batch[3/8] - loss: 3.423594  accuracy: 53.1250%(68/128)\n",
            "Batch[4/8] - loss: 3.255450  accuracy: 53.1250%(68/128)\n",
            "Batch[5/8] - loss: 3.633130  accuracy: 53.9062%(69/128)\n",
            "Batch[6/8] - loss: 2.825797  accuracy: 53.1250%(68/128)\n",
            "Batch[7/8] - loss: 2.726923  accuracy: 48.4375%(62/128)\n",
            "Batch[8/8] - loss: 2.495608  accuracy: 54.6875%(70/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.83582   0.87500   0.85496        64\n",
            "          FR    0.86885   0.82812   0.84800        64\n",
            "\n",
            "    accuracy                        0.85156       128\n",
            "   macro avg    0.85234   0.85156   0.85148       128\n",
            "weighted avg    0.85234   0.85156   0.85148       128\n",
            "\n",
            "Val set acc: 0.8515625\n",
            "Best val set acc: 0.8515625\n",
            "\n",
            "Epoch  2 / 15\n",
            "Batch[1/8] - loss: 1.877543  accuracy: 66.4062%(85/128)\n",
            "Batch[2/8] - loss: 1.607342  accuracy: 64.0625%(82/128)\n",
            "Batch[3/8] - loss: 1.846256  accuracy: 64.0625%(82/128)\n",
            "Batch[4/8] - loss: 1.582360  accuracy: 67.1875%(86/128)\n",
            "Batch[5/8] - loss: 1.845252  accuracy: 65.6250%(84/128)\n",
            "Batch[6/8] - loss: 1.838338  accuracy: 60.1562%(77/128)\n",
            "Batch[7/8] - loss: 1.832111  accuracy: 57.8125%(74/128)\n",
            "Batch[8/8] - loss: 1.445127  accuracy: 62.5000%(80/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.94915   0.87500   0.91057        64\n",
            "          FR    0.88406   0.95312   0.91729        64\n",
            "\n",
            "    accuracy                        0.91406       128\n",
            "   macro avg    0.91661   0.91406   0.91393       128\n",
            "weighted avg    0.91661   0.91406   0.91393       128\n",
            "\n",
            "Val set acc: 0.9140625\n",
            "Best val set acc: 0.9140625\n",
            "\n",
            "Epoch  3 / 15\n",
            "Batch[1/8] - loss: 1.392187  accuracy: 63.2812%(81/128)\n",
            "Batch[2/8] - loss: 1.433878  accuracy: 67.1875%(86/128)\n",
            "Batch[3/8] - loss: 1.048788  accuracy: 69.5312%(89/128)\n",
            "Batch[4/8] - loss: 1.214885  accuracy: 73.4375%(94/128)\n",
            "Batch[5/8] - loss: 0.829074  accuracy: 74.2188%(95/128)\n",
            "Batch[6/8] - loss: 0.659488  accuracy: 77.3438%(99/128)\n",
            "Batch[7/8] - loss: 1.278881  accuracy: 70.3125%(90/128)\n",
            "Batch[8/8] - loss: 1.048683  accuracy: 72.6562%(93/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    1.00000   0.84375   0.91525        64\n",
            "          FR    0.86486   1.00000   0.92754        64\n",
            "\n",
            "    accuracy                        0.92188       128\n",
            "   macro avg    0.93243   0.92188   0.92140       128\n",
            "weighted avg    0.93243   0.92188   0.92140       128\n",
            "\n",
            "Val set acc: 0.921875\n",
            "Best val set acc: 0.921875\n",
            "\n",
            "Epoch  4 / 15\n",
            "Batch[1/8] - loss: 0.717982  accuracy: 82.0312%(105/128)\n",
            "Batch[2/8] - loss: 0.864308  accuracy: 74.2188%(95/128)\n",
            "Batch[3/8] - loss: 0.603758  accuracy: 82.0312%(105/128)\n",
            "Batch[4/8] - loss: 0.610676  accuracy: 79.6875%(102/128)\n",
            "Batch[5/8] - loss: 0.914144  accuracy: 75.0000%(96/128)\n",
            "Batch[6/8] - loss: 0.491615  accuracy: 83.5938%(107/128)\n",
            "Batch[7/8] - loss: 0.787935  accuracy: 78.9062%(101/128)\n",
            "Batch[8/8] - loss: 0.765551  accuracy: 81.2500%(104/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    1.00000   0.78125   0.87719        64\n",
            "          FR    0.82051   1.00000   0.90141        64\n",
            "\n",
            "    accuracy                        0.89062       128\n",
            "   macro avg    0.91026   0.89062   0.88930       128\n",
            "weighted avg    0.91026   0.89062   0.88930       128\n",
            "\n",
            "Val set acc: 0.890625\n",
            "Best val set acc: 0.921875\n",
            "\n",
            "Epoch  5 / 15\n",
            "Batch[1/8] - loss: 0.495938  accuracy: 83.5938%(107/128)\n",
            "Batch[2/8] - loss: 0.884168  accuracy: 78.9062%(101/128)\n",
            "Batch[3/8] - loss: 0.458680  accuracy: 87.5000%(112/128)\n",
            "Batch[4/8] - loss: 0.567042  accuracy: 83.5938%(107/128)\n",
            "Batch[5/8] - loss: 0.651939  accuracy: 84.3750%(108/128)\n",
            "Batch[6/8] - loss: 0.348816  accuracy: 88.2812%(113/128)\n",
            "Batch[7/8] - loss: 0.481402  accuracy: 85.9375%(110/128)\n",
            "Batch[8/8] - loss: 0.212938  accuracy: 92.9688%(119/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96610   0.89062   0.92683        64\n",
            "          FR    0.89855   0.96875   0.93233        64\n",
            "\n",
            "    accuracy                        0.92969       128\n",
            "   macro avg    0.93233   0.92969   0.92958       128\n",
            "weighted avg    0.93233   0.92969   0.92958       128\n",
            "\n",
            "Val set acc: 0.9296875\n",
            "Best val set acc: 0.9296875\n",
            "\n",
            "Epoch  6 / 15\n",
            "Batch[1/8] - loss: 0.426630  accuracy: 85.1562%(109/128)\n",
            "Batch[2/8] - loss: 0.477828  accuracy: 84.3750%(108/128)\n",
            "Batch[3/8] - loss: 0.317775  accuracy: 87.5000%(112/128)\n",
            "Batch[4/8] - loss: 0.507420  accuracy: 85.9375%(110/128)\n",
            "Batch[5/8] - loss: 0.500508  accuracy: 81.2500%(104/128)\n",
            "Batch[6/8] - loss: 0.123019  accuracy: 92.1875%(118/128)\n",
            "Batch[7/8] - loss: 0.274278  accuracy: 85.1562%(109/128)\n",
            "Batch[8/8] - loss: 0.148390  accuracy: 95.3125%(122/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    1.00000   0.79688   0.88696        64\n",
            "          FR    0.83117   1.00000   0.90780        64\n",
            "\n",
            "    accuracy                        0.89844       128\n",
            "   macro avg    0.91558   0.89844   0.89738       128\n",
            "weighted avg    0.91558   0.89844   0.89738       128\n",
            "\n",
            "Val set acc: 0.8984375\n",
            "Best val set acc: 0.9296875\n",
            "\n",
            "Epoch  7 / 15\n",
            "Batch[1/8] - loss: 0.175935  accuracy: 93.7500%(120/128)\n",
            "Batch[2/8] - loss: 0.298917  accuracy: 85.1562%(109/128)\n",
            "Batch[3/8] - loss: 0.300774  accuracy: 93.7500%(120/128)\n",
            "Batch[4/8] - loss: 0.245836  accuracy: 90.6250%(116/128)\n",
            "Batch[5/8] - loss: 0.445758  accuracy: 89.0625%(114/128)\n",
            "Batch[6/8] - loss: 0.097451  accuracy: 96.0938%(123/128)\n",
            "Batch[7/8] - loss: 0.280424  accuracy: 90.6250%(116/128)\n",
            "Batch[8/8] - loss: 0.123588  accuracy: 95.3125%(122/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.98214   0.85938   0.91667        64\n",
            "          FR    0.87500   0.98438   0.92647        64\n",
            "\n",
            "    accuracy                        0.92188       128\n",
            "   macro avg    0.92857   0.92188   0.92157       128\n",
            "weighted avg    0.92857   0.92188   0.92157       128\n",
            "\n",
            "Val set acc: 0.921875\n",
            "Best val set acc: 0.9296875\n",
            "\n",
            "Epoch  8 / 15\n",
            "Batch[1/8] - loss: 0.197423  accuracy: 95.3125%(122/128)\n",
            "Batch[2/8] - loss: 0.253274  accuracy: 91.4062%(117/128)\n",
            "Batch[3/8] - loss: 0.167728  accuracy: 95.3125%(122/128)\n",
            "Batch[4/8] - loss: 0.275226  accuracy: 89.8438%(115/128)\n",
            "Batch[5/8] - loss: 0.345524  accuracy: 91.4062%(117/128)\n",
            "Batch[6/8] - loss: 0.096833  accuracy: 96.0938%(123/128)\n",
            "Batch[7/8] - loss: 0.186237  accuracy: 94.5312%(121/128)\n",
            "Batch[8/8] - loss: 0.262673  accuracy: 93.7500%(120/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.95161   0.92188   0.93651        64\n",
            "          FR    0.92424   0.95312   0.93846        64\n",
            "\n",
            "    accuracy                        0.93750       128\n",
            "   macro avg    0.93793   0.93750   0.93748       128\n",
            "weighted avg    0.93793   0.93750   0.93748       128\n",
            "\n",
            "Val set acc: 0.9375\n",
            "Best val set acc: 0.9375\n",
            "\n",
            "Epoch  9 / 15\n",
            "Batch[1/8] - loss: 0.127739  accuracy: 95.3125%(122/128)\n",
            "Batch[2/8] - loss: 0.427709  accuracy: 88.2812%(113/128)\n",
            "Batch[3/8] - loss: 0.248169  accuracy: 93.7500%(120/128)\n",
            "Batch[4/8] - loss: 0.120806  accuracy: 96.0938%(123/128)\n",
            "Batch[5/8] - loss: 0.288085  accuracy: 92.1875%(118/128)\n",
            "Batch[6/8] - loss: 0.047646  accuracy: 98.4375%(126/128)\n",
            "Batch[7/8] - loss: 0.175060  accuracy: 95.3125%(122/128)\n",
            "Batch[8/8] - loss: 0.341100  accuracy: 91.4062%(117/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.98246   0.87500   0.92562        64\n",
            "          FR    0.88732   0.98438   0.93333        64\n",
            "\n",
            "    accuracy                        0.92969       128\n",
            "   macro avg    0.93489   0.92969   0.92948       128\n",
            "weighted avg    0.93489   0.92969   0.92948       128\n",
            "\n",
            "Val set acc: 0.9296875\n",
            "Best val set acc: 0.9375\n",
            "\n",
            "Epoch  10 / 15\n",
            "Batch[1/8] - loss: 0.158311  accuracy: 96.0938%(123/128)\n",
            "Batch[2/8] - loss: 0.109798  accuracy: 95.3125%(122/128)\n",
            "Batch[3/8] - loss: 0.108599  accuracy: 96.8750%(124/128)\n",
            "Batch[4/8] - loss: 0.081442  accuracy: 96.0938%(123/128)\n",
            "Batch[5/8] - loss: 0.112784  accuracy: 96.8750%(124/128)\n",
            "Batch[6/8] - loss: 0.017149  accuracy: 100.0000%(128/128)\n",
            "Batch[7/8] - loss: 0.138165  accuracy: 96.0938%(123/128)\n",
            "Batch[8/8] - loss: 0.051468  accuracy: 98.4375%(126/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.98333   0.92188   0.95161        64\n",
            "          FR    0.92647   0.98438   0.95455        64\n",
            "\n",
            "    accuracy                        0.95312       128\n",
            "   macro avg    0.95490   0.95312   0.95308       128\n",
            "weighted avg    0.95490   0.95312   0.95308       128\n",
            "\n",
            "Val set acc: 0.953125\n",
            "Best val set acc: 0.953125\n",
            "\n",
            "Epoch  11 / 15\n",
            "Batch[1/8] - loss: 0.174948  accuracy: 92.9688%(119/128)\n",
            "Batch[2/8] - loss: 0.148346  accuracy: 95.3125%(122/128)\n",
            "Batch[3/8] - loss: 0.070165  accuracy: 96.8750%(124/128)\n",
            "Batch[4/8] - loss: 0.127570  accuracy: 96.0938%(123/128)\n",
            "Batch[5/8] - loss: 0.140468  accuracy: 93.7500%(120/128)\n",
            "Batch[6/8] - loss: 0.028306  accuracy: 98.4375%(126/128)\n",
            "Batch[7/8] - loss: 0.051066  accuracy: 96.8750%(124/128)\n",
            "Batch[8/8] - loss: 0.163067  accuracy: 95.3125%(122/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.98276   0.89062   0.93443        64\n",
            "          FR    0.90000   0.98438   0.94030        64\n",
            "\n",
            "    accuracy                        0.93750       128\n",
            "   macro avg    0.94138   0.93750   0.93736       128\n",
            "weighted avg    0.94138   0.93750   0.93736       128\n",
            "\n",
            "Val set acc: 0.9375\n",
            "Best val set acc: 0.953125\n",
            "\n",
            "Epoch  12 / 15\n",
            "Batch[1/8] - loss: 0.025645  accuracy: 98.4375%(126/128)\n",
            "Batch[2/8] - loss: 0.023169  accuracy: 100.0000%(128/128)\n",
            "Batch[3/8] - loss: 0.075719  accuracy: 95.3125%(122/128)\n",
            "Batch[4/8] - loss: 0.043125  accuracy: 98.4375%(126/128)\n",
            "Batch[5/8] - loss: 0.235198  accuracy: 92.9688%(119/128)\n",
            "Batch[6/8] - loss: 0.064375  accuracy: 98.4375%(126/128)\n",
            "Batch[7/8] - loss: 0.054759  accuracy: 98.4375%(126/128)\n",
            "Batch[8/8] - loss: 0.088111  accuracy: 96.8750%(124/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96721   0.92188   0.94400        64\n",
            "          FR    0.92537   0.96875   0.94656        64\n",
            "\n",
            "    accuracy                        0.94531       128\n",
            "   macro avg    0.94629   0.94531   0.94528       128\n",
            "weighted avg    0.94629   0.94531   0.94528       128\n",
            "\n",
            "Val set acc: 0.9453125\n",
            "Best val set acc: 0.953125\n",
            "\n",
            "Epoch  13 / 15\n",
            "Batch[1/8] - loss: 0.035068  accuracy: 99.2188%(127/128)\n",
            "Batch[2/8] - loss: 0.130100  accuracy: 94.5312%(121/128)\n",
            "Batch[3/8] - loss: 0.040615  accuracy: 97.6562%(125/128)\n",
            "Batch[4/8] - loss: 0.017204  accuracy: 100.0000%(128/128)\n",
            "Batch[5/8] - loss: 0.151631  accuracy: 93.7500%(120/128)\n",
            "Batch[6/8] - loss: 0.010523  accuracy: 100.0000%(128/128)\n",
            "Batch[7/8] - loss: 0.118746  accuracy: 96.0938%(123/128)\n",
            "Batch[8/8] - loss: 0.081475  accuracy: 96.0938%(123/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.98333   0.92188   0.95161        64\n",
            "          FR    0.92647   0.98438   0.95455        64\n",
            "\n",
            "    accuracy                        0.95312       128\n",
            "   macro avg    0.95490   0.95312   0.95308       128\n",
            "weighted avg    0.95490   0.95312   0.95308       128\n",
            "\n",
            "Val set acc: 0.953125\n",
            "Best val set acc: 0.953125\n",
            "\n",
            "Epoch  14 / 15\n",
            "Batch[1/8] - loss: 0.066493  accuracy: 96.0938%(123/128)\n",
            "Batch[2/8] - loss: 0.063680  accuracy: 96.8750%(124/128)\n",
            "Batch[3/8] - loss: 0.040986  accuracy: 97.6562%(125/128)\n",
            "Batch[4/8] - loss: 0.050638  accuracy: 97.6562%(125/128)\n",
            "Batch[5/8] - loss: 0.072302  accuracy: 96.8750%(124/128)\n",
            "Batch[6/8] - loss: 0.050672  accuracy: 98.4375%(126/128)\n",
            "Batch[7/8] - loss: 0.006039  accuracy: 100.0000%(128/128)\n",
            "Batch[8/8] - loss: 0.062503  accuracy: 97.6562%(125/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96774   0.93750   0.95238        64\n",
            "          FR    0.93939   0.96875   0.95385        64\n",
            "\n",
            "    accuracy                        0.95312       128\n",
            "   macro avg    0.95357   0.95312   0.95311       128\n",
            "weighted avg    0.95357   0.95312   0.95311       128\n",
            "\n",
            "Val set acc: 0.953125\n",
            "Best val set acc: 0.953125\n",
            "\n",
            "Epoch  15 / 15\n",
            "Batch[1/8] - loss: 0.053018  accuracy: 96.8750%(124/128)\n",
            "Batch[2/8] - loss: 0.096972  accuracy: 94.5312%(121/128)\n",
            "Batch[3/8] - loss: 0.045665  accuracy: 98.4375%(126/128)\n",
            "Batch[4/8] - loss: 0.060818  accuracy: 96.8750%(124/128)\n",
            "Batch[5/8] - loss: 0.052761  accuracy: 98.4375%(126/128)\n",
            "Batch[6/8] - loss: 0.009417  accuracy: 99.2188%(127/128)\n",
            "Batch[7/8] - loss: 0.067754  accuracy: 96.8750%(124/128)\n",
            "Batch[8/8] - loss: 0.148128  accuracy: 96.0938%(123/128)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96774   0.93750   0.95238        64\n",
            "          FR    0.93939   0.96875   0.95385        64\n",
            "\n",
            "    accuracy                        0.95312       128\n",
            "   macro avg    0.95357   0.95312   0.95311       128\n",
            "weighted avg    0.95357   0.95312   0.95311       128\n",
            "\n",
            "Val set acc: 0.953125\n",
            "Best val set acc: 0.953125\n",
            "NR {'precision': 1.0, 'recall': 0.96875, 'f1-score': 0.9841269841269841, 'support': 64.0}\n",
            "FR {'precision': 0.9696969696969697, 'recall': 1.0, 'f1-score': 0.9846153846153847, 'support': 64.0}\n",
            "accuracy 0.984375\n",
            "macro avg {'precision': 0.9848484848484849, 'recall': 0.984375, 'f1-score': 0.9843711843711844, 'support': 128.0}\n",
            "weighted avg {'precision': 0.9848484848484849, 'recall': 0.984375, 'f1-score': 0.9843711843711844, 'support': 128.0}\n",
            "result:0.9844\n",
            "{'NR': {'precision': 1.0, 'recall': 0.96875, 'f1-score': 0.9841269841269841, 'support': 64.0}, 'FR': {'precision': 0.9696969696969697, 'recall': 1.0, 'f1-score': 0.9846153846153847, 'support': 64.0}, 'accuracy': 0.984375, 'macro avg': {'precision': 0.9848484848484849, 'recall': 0.984375, 'f1-score': 0.9843711843711844, 'support': 128.0}, 'weighted avg': {'precision': 0.9848484848484849, 'recall': 0.984375, 'f1-score': 0.9843711843711844, 'support': 128.0}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NR': {'precision': 1.0,\n",
              "  'recall': 0.96875,\n",
              "  'f1-score': 0.9841269841269841,\n",
              "  'support': 64.0},\n",
              " 'FR': {'precision': 0.9696969696969697,\n",
              "  'recall': 1.0,\n",
              "  'f1-score': 0.9846153846153847,\n",
              "  'support': 64.0},\n",
              " 'accuracy': 0.984375,\n",
              " 'macro avg': {'precision': 0.9848484848484849,\n",
              "  'recall': 0.984375,\n",
              "  'f1-score': 0.9843711843711844,\n",
              "  'support': 128.0},\n",
              " 'weighted avg': {'precision': 0.9848484848484849,\n",
              "  'recall': 0.984375,\n",
              "  'f1-score': 0.9843711843711844,\n",
              "  'support': 128.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "model = Classifier()\n",
        "train_and_test(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}