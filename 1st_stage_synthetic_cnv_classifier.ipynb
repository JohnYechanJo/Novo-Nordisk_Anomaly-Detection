{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "stage": {}
      },
      "source": [
        "<a href=\"https://colab.research.google.com/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title",
        "stage": {}
      },
      "source": [
        "# Synthetic CNV Image Classifier\n",
        "\n",
        "이 노트북은 OCT2017 데이터셋의 CNV 이미지를 사용하여 Stable Diffusion 모델로 Synthetic CNV 이미지를 생성하고, 이를 Normal CNV 이미지와 다양한 비율(0%~100%)로 혼합하여 Classifier를 학습시킵니다. 목표는 Synthetic 이미지의 최적 비율을 찾아 Classifier 성능을 최대화하는 것입니다.\n",
        "\n",
        "**실행 순서**:\n",
        "1. 데이터 전처리: OCT2017 CNV/NORMAL 이미지 → ViT 임베딩 → `pre-trained_dataset.pt`.\n",
        "2. Diffusion Model 파인튜닝: Stable Diffusion UNet을 CNV 이미지로 학습.\n",
        "3. Synthetic CNV 이미지 생성: 파인튜닝된 모델로 이미지 생성 → ViT 임베딩 → `synthetic_cnv_dataset.pt`.\n",
        "4. 비율별 Classifier 학습: Synthetic 비율 0%~100%로 데이터 혼합 → Classifier 학습 → 성능 비교.\n",
        "\n",
        "**실행 환경**: Google Colab (GPU, 예: T4 또는 A100 권장)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup",
        "stage": {}
      },
      "source": [
        "## 1. 환경 설정 및 패키지 설치\n",
        "\n",
        "필요한 Python 패키지를 설치하고, 랜덤 시드를 설정하여 재현성을 보장합니다. GPU를 사용하며, 메모리 관리를 위한 함수도 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63b91c-30f4-41a0-e533-602ba2b07e86",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━()]
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# 메모리 정리 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess",
        "stage": {}
      },
      "source": [
        "## 2. 데이터 전처리: OCT2017 데이터셋 → ViT 임베딩\n",
        "\n",
        "OCT2017 데이터셋의 CNV 및 NORMAL 이미지를 로드하고, ViT 모델을 사용하여 임베딩을 생성합니다. 결과는 `pre-trained_dataset.pt` 파일로 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5dc3a3-d8a6-4f2b-90d4-8f2531a22dbd",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing: Start!\n"
          ]
        }
      ],
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400))),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def vit_process(img_list, batch_size=16):\n",
        "    model_name = \"google/vit-base-patch16-224\"\n",
        "    vit_model = ViTModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    vit_model.eval()\n",
        "    length = len(img_list)\n",
        "    layers_0, layers_1, layers_2, layers_3 = [], [], [], []\n",
        "    for i in range(int(length/batch_size)):\n",
        "        batch_imgs = img_list[i*batch_size:(i+1)*batch_size]\n",
        "        batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = vit_model(pixel_values=batch_tensor)\n",
        "        hidden = outputs.hidden_states\n",
        "        last_layer = hidden[12][:,0,:]\n",
        "        hidden_layer_1 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_2 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_3 = torch.zeros_like(last_layer)\n",
        "        for j in range(12):\n",
        "            if j < 4:\n",
        "                hidden_layer_1 += hidden[j][:,0,:]/4\n",
        "            elif j < 8:\n",
        "                hidden_layer_2 += hidden[j][:,0,:]/4\n",
        "            else:\n",
        "                hidden_layer_3 += hidden[j][:,0,:]/4\n",
        "        layers_0.append(last_layer)\n",
        "        layers_1.append(hidden_layer_1)\n",
        "        layers_2.append(hidden_layer_2)\n",
        "        layers_3.append(hidden F
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diffusion_model",
        "stage": {}
      },
      "source": [
        "## 3. Diffusion Model 파인튜닝\n",
        "\n",
        "Stable Diffusion 모델의 UNet을 CNV 이미지로 파인튜닝합니다. Kaggle 데이터셋을 다운로드하고, CNV 이미지를 전처리하여 학습에 사용합니다. 학습된 모델은 `/content/models/sd_cnv_finetuned`에 저장됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esp_rmbJVypT",
        "stage": {}
      },
      "source": [
        "### 3.1F
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsP2af82VypT",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_cnv_images():\n",
        "    # Kaggle 데이터셋 다운로드\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # 데이터셋 경로 설정 및 CNV 이미지 복사\n",
        "    in_dir = os.path.join(path, \"OCT2017 /train/CNV\")\n",
        "    out_dir = \"/content/processed/CNV/\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for fn in os.listdir(in_dir):\n",
        "        img = Image.open(os.path.join(in_dir, fn)).convert(\"RGB\")\n",
        "        img = img.resize((512, 512), resample=Image.LANCZOS)\n",
        "        img.save(os.path.join(out_dir, fn))\n",
        "    print(f\"Processed CNV images saved to {out_dir}\")\n",
        "\n",
        "prepare_cnv_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jyYOAE4VypT",
        "stage": {}
      },
      "source": [
        "### 3.2 데이터셋 클래스 정의\n",
        "\n",
        "`ImgDataset` 클래스를 정의하여 CNV 이미지를 로드하고, 학습에 필요한 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZnwvqajVypU",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, root_dir:str, tokenizer, cohort:str = \"CNV\", resolution=512, max_length=77, dataset_size:int = 6400) -> None:\n",
        "        self.files = [\n",
        "            os.path.join(root_dir, f)\n",
        "            for f in os.listdir(root_dir)\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ]\n",
        "        if len(self.files) > dataset_size: self.files = self.files[:dataset_size]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "        self.prompt = f\"OCT scan showing {cohort}\"\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> dict:\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze(0)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3NQWEOLVypU",
        "stage": {}
      },
      "source": [
        "### 3.3 학습 함수 정의\n",
        "\n",
        "`train` 함수를 정의하여 Stable Diffusion UNet을 CNV 이미지로 파인튜닝합니다. 혼합 정밀도 학습과 체크포인트 저장을 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w36I0hc5VypU",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "def train(\n",
        "    pretrained_model: str,\n",
        "    data_dir: str,\n",
        "    output_dir: str = \"sd_cnv_finetuned\",\n",
        "    cohort: str = \"CNV\",\n",
        "    resolution: int = 512,\n",
        "    dataset_size: int = 6400,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-4,\n",
        "    epochs: int = 5,\n",
        "    grad_accum_steps: int = 1,\n",
        "    save_steps: int = 1000,\n",
        "    resume_checkpoint: str = None,\n",
        "    unet: UNet2DConditionModel = None,\n",
        "    accelerator: Accelerator = None,\n",
        "):\n",
        "    # 1) Prepare output directory, accelerator & device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    accel = accelerator or Accelerator()\n",
        "    device = accel.device\n",
        "\n",
        "    # 2) Load or resume UNet\n",
        "    if resume_checkpoint and unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(resume_checkpoint).to(device)\n",
        "    elif unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
        "\n",
        "    # 3) Load & freeze tokenizer + text encoder\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
        "    text_encoder.requires_grad_(False)\n",
        "\n",
        "    # 4) Load & freeze VAE\n",
        "    vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
        "    vae.requires_grad_(False)\n",
        "\n",
        "    # 5) Load noise scheduler\n",
        "    scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
        "\n",
        "    # 6) Prepare dataset & dataloader\n",
        "    dataset = ImgDataset(data_dir, tokenizer, cohort=cohort, resolution=resolution, dataset_size=dataset_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 7) Optimizer (only UNet params)\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 8) Wrap models, optimizer, and dataloader for mixed-precision / distributed\n",
        "    unet, optimizer, dataloader = accel.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # 9) Resume state if requested\n",
        "    if resume_checkpoint:\n",
        "        accel.load_state(resume_checkpoint)\n",
        "        global_step = int(resume_checkpoint.rsplit(\"_\", 1)[-1])\n",
        "    else:\n",
        "        global_step = 0\n",
        "\n",
        "    # 10) Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accel.accumulate(unet):\n",
        "                # Encode images to latents\n",
        "                pixels = batch[\"pixel_values\"].to(device)\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * 0.18215\n",
        "\n",
        "                # Add noise\n",
        "                noise = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
        "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Text conditioning\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "                # Noise prediction & loss\n",
        "                pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "\n",
        "                # Backpropagate\n",
        "                accel.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % save_steps == 0:\n",
        "                accel.wait_for_everyone()\n",
        "                ckpt_dir = os.path.join(output_dir, f\"checkpoint_{global_step}\")\n",
        "                unet.save_pretrained(ckpt_dir)\n",
        "                if accel.is_main_process:\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "                accel.save_state(ckpt_dir)\n",
In the provided JSON document, I noticed that some code cells are truncated (e.g., the `vit_process` function in the data preprocessing section). To ensure the `.ipynb` file is complete and functional, I'll reconstruct the missing parts based on the context and typical implementations. Below, I continue from where the previous response left off, completing the truncated code and including all remaining cells with the `"stage": {}` metadata added.

<xaiArtifact artifact_id="12783a1c-2741-4e57-af81-7f7776c8fd54" artifact_version_id="1cd0b8f2-bb08-475b-9be5-4d0dfdf142f0" title="1st_stage_synthetic_cnv_classifier.ipynb" contentType="application/x-ipynb+json">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "stage": {}
      },
      "source": [
        "<a href=\"https://colab.research.google.com/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title",
        "stage": {}
      },
      "source": [
        "# Synthetic CNV Image Classifier\n",
        "\n",
        "이 노트북은 OCT2017 데이터셋의 CNV 이미지를 사용하여 Stable Diffusion 모델로 Synthetic CNV 이미지를 생성하고, 이를 Normal CNV 이미지와 다양한 비율(0%~100%)로 혼합하여 Classifier를 학습시킵니다. 목표는 Synthetic 이미지의 최적 비율을 찾아 Classifier 성능을 최대화하는 것입니다.\n",
        "\n",
        "**실행 순서**:\n",
        "1. 데이터 전처리: OCT2017 CNV/NORMAL 이미지 → ViT 임베딩 → `pre-trained_dataset.pt`.\n",
        "2. Diffusion Model 파인튜닝: Stable Diffusion UNet을 CNV 이미지로 학습.\n",
        "3. Synthetic CNV 이미지 생성: 파인튜닝된 모델로 이미지 생성 → ViT 임베딩 → `synthetic_cnv_dataset.pt`.\n",
        "4. 비율별 Classifier 학습: Synthetic 비율 0%~100%로 데이터 혼합 → Classifier 학습 → 성능 비교.\n",
        "\n",
        "**실행 환경**: Google Colab (GPU, 예: T4 또는 A100 권장)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup",
        "stage": {}
      },
      "source": [
        "## 1. 환경 설정 및 패키지 설치\n",
        "\n",
        "필요한 Python 패키지를 설치하고, 랜덤 시드를 설정하여 재현성을 보장합니다. GPU를 사용하며, 메모리 관리를 위한 함수도 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63b91c-30f4-41a0-e533-602ba2b07e86",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "... (출력 내용은 동일하므로 생략)"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# 메모리 정리 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess",
        "stage": {}
      },
      "source": [
        "## 2. 데이터 전처리: OCT2017 데이터셋 → ViT 임베딩\n",
        "\n",
        "OCT2017 데이터셋의 CNV 및 NORMAL 이미지를 로드하고, ViT 모델을 사용하여 임베딩을 생성합니다. 결과는 `pre-trained_dataset.pt` 파일로 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5dc3a3-d8a6-4f2b-90d4-8f2531a22dbd",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing: Start!\n"
          ]
        }
      ],
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400))),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def vit_process(img_list, batch_size=16):\n",
        "    model_name = \"google/vit-base-patch16-224\"\n",
        "    vit_model = ViTModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    vit_model.eval()\n",
        "    length = len(img_list)\n",
        "    layers_0, layers_1, layers_2, layers_3 = [], [], [], []\n",
        "    for i in range(int(length/batch_size)):\n",
        "        batch_imgs = img_list[i*batch_size:(i+1)*batch_size]\n",
        "        batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = vit_model(pixel_values=batch_tensor)\n",
        "        hidden = outputs.hidden_states\n",
        "        last_layer = hidden[12][:,0,:]\n",
        "        hidden_layer_1 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_2 = torch.zeros_like(last_layer)\n",
        "        hidden_layer_3 = torch.zeros_like(last_layer)\n",
        "        for j in range(12):\n",
        "            if j < 4:\n",
        "                hidden_layer_1 += hidden[j][:,0,:]/4\n",
        "            elif j < 8:\n",
        "                hidden_layer_2 += hidden[j][:,0,:]/4\n",
        "            else:\n",
        "                hidden_layer_3 += hidden[j][:,0,:]/4\n",
        "        layers_0.append(last_layer)\n",
        "        layers_1.append(hidden_layer_1)\n",
        "        layers_2.append(hidden_layer_2)\n",
        "        layers_3.append(hidden_layer_3)\n",
        "        clear_memory()\n",
        "    return (torch.cat(layers_0, dim=0), torch.cat(layers_1, dim=0),\n",
        "            torch.cat(layers_2, dim=0), torch.cat(layers_3, dim=0))\n",
        "\n",
        "def load_trans(path, pic_num=640):\n",
        "    trans_toTensor = img_transform()\n",
        "    image_list = []\n",
        "    i = 0\n",
        "    for filename in os.listdir(path):\n",
        "        if i == pic_num:\n",
        "            break\n",
        "        file_path = os.path.join(path, filename)\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            try:\n",
        "                img = Image.open(file_path).convert(\"RGB\")\n",
        "                tensor_img = trans_toTensor(img)\n",
        "                image_list.append(tensor_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Skip: {filename}, Error: {e}\")\n",
        "        i += 1\n",
        "    return image_list\n",
        "\n",
        "def preprocess_data():\n",
        "    print(\"Data Preprocessing: Start!\")\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    loadpath = os.path.join(path, \"OCT2017 /train\")\n",
        "    train_path_cnv = os.path.join(loadpath, \"CNV\")\n",
        "    train_path_normal = os.path.join(loadpath, \"NORMAL\")\n",
        "\n",
        "    cnv_tensor_list = load_trans(train_path_cnv)\n",
        "    normal_tensor_list = load_trans(train_path_normal)\n",
        "    out_00, out_01, out_02, out_03 = vit_process(cnv_tensor_list)\n",
        "    out_10, out_11, out_12, out_13 = vit_process(normal_tensor_list)\n",
        "\n",
        "    batch_size, batch_num, half_batch = 128, 10, 64\n",
        "    tensor_list_0, tensor_list_1, tensor_list_2, tensor_list_3, labels_list = [], [], [], [], []\n",
        "    for i in range(batch_num):\n",
        "        cnv_tensor_0 = out_00[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_0 = out_10[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_1 = out_01[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_1 = out_11[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_2 = out_02[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_2 = out_12[i*half_batch:(i+1)*half_batch]\n",
        "        cnv_tensor_3 = out_03[i*half_batch:(i+1)*half_batch]\n",
        "        norm_tensor_3 = out_13[i*half_batch:(i+1)*half_batch]\n",
        "        tensor_0 = torch.cat((cnv_tensor_0, norm_tensor_0), dim=0)\n",
        "        tensor_1 = torch.cat((cnv_tensor_1, norm_tensor_1), dim=0)\n",
        "        tensor_2 = torch.cat((cnv_tensor_2, norm_tensor_2), dim=0)\n",
        "        tensor_3 = torch.cat((cnv_tensor_3, norm_tensor_3), dim=0)\n",
        "        labels = torch.cat([torch.zeros(half_batch, dtype=torch.long), torch.ones(half_batch, dtype=torch.long)], dim=0)\n",
        "        indices = torch.randperm(batch_size)\n",
        "        tensor_list_0.append(tensor_0[indices])\n",
        "        tensor_list_1.append(tensor_1[indices])\n",
        "        tensor_list_2.append(tensor_2[indices])\n",
        "        tensor_list_3.append(tensor_3[indices])\n",
        "        labels_list.append(labels[indices])\n",
        "    tensor_set_0 = torch.cat(tensor_list_0, dim=0)\n",
        "    tensor_set_1 = torch.cat(tensor_list_1, dim=0)\n",
        "    tensor_set_2 = torch.cat(tensor_list_2, dim=0)\n",
        "    tensor_set_3 = torch.cat(tensor_list_3, dim=0)\n",
        "    labels_set = torch.cat(labels_list, dim=0)\n",
        "\n",
        "    if os.path.exists('pre-trained_dataset.pt'):\n",
        "        os.remove('pre-trained_dataset.pt')\n",
        "    torch.save({'data_0': tensor_set_0, 'data_1': tensor_set_1, 'data_2': tensor_set_2, 'data_3': tensor_set_3, 'label': labels_set}, 'pre-trained_dataset.pt')\n",
        "    print('Data Preprocessing: Done!')\n",
        "\n",
        "preprocess_data()\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diffusion_model",
        "stage": {}
      },
      "source": [
        "## 3. Diffusion Model 파인튜닝\n",
        "\n",
        "Stable Diffusion 모델의 UNet을 CNV 이미지로 파인튜닝합니다. Kaggle 데이터셋을 다운로드하고, CNV 이미지를 전처리하여 학습에 사용합니다. 학습된 모델은 `/content/models/sd_cnv_finetuned`에 저장됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esp_rmbJVypT",
        "stage": {}
      },
      "source": [
        "### 3.1 데이터 준비\n",
        "\n",
        "Kaggle 데이터셋에서 CNV 이미지를 다운로드하고, 512x512 크기로 리사이징하여 `/content/processed/CNV/`에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsP2af82VypT",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_cnv_images():\n",
        "    # Kaggle 데이터셋 다운로드\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # 데이터셋 경로 설정 및 CNV 이미지 복사\n",
        "    in_dir = os.path.join(path, \"OCT2017 /train/CNV\")\n",
        "    out_dir = \"/content/processed/CNV/\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for fn in os.listdir(in_dir):\n",
        "        img = Image.open(os.path.join(in_dir, fn)).convert(\"RGB\")\n",
        "        img = img.resize((512, 512), resample=Image.LANCZOS)\n",
        "        img.save(os.path.join(out_dir, fn))\n",
        "    print(f\"Processed CNV images saved to {out_dir}\")\n",
        "\n",
        "prepare_cnv_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jyYOAE4VypT",
        "stage": {}
      },
      "source": [
        "### 3.2 데이터셋 클래스 정의\n",
        "\n",
        "`ImgDataset` 클래스를 정의하여 CNV 이미지를 로드하고, 학습에 필요한 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZnwvqajVypU",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, root_dir:str, tokenizer, cohort:str = \"CNV\", resolution=512, max_length=77, dataset_size:int = 6400) -> None:\n",
        "        self.files = [\n",
        "            os.path.join(root_dir, f)\n",
        "            for f in os.listdir(root_dir)\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ]\n",
        "        if len(self.files) > dataset_size: self.files = self.files[:dataset_size]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "        self.prompt = f\"OCT scan showing {cohort}\"\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> dict:\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze(0)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3NQWEOLVypU",
        "stage": {}
      },
      "source": [
        "### 3.3 학습 함수 정의\n",
        "\n",
        "`train` 함수를 정의하여 Stable Diffusion UNet을 CNV 이미지로 파인튜닝합니다. 혼합 정밀도 학습과 체크포인트 저장을 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w36I0hc5VypU",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "def train(\n",
        "    pretrained_model: str,\n",
        "    data_dir: str,\n",
        "    output_dir: str = \"sd_cnv_finetuned\",\n",
        "    cohort: str = \"CNV\",\n",
        "    resolution: int = 512,\n",
        "    dataset_size: int = 6400,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-4,\n",
        "    epochs: int = 5,\n",
        "    grad_accum_steps: int = 1,\n",
        "    save_steps: int = 1000,\n",
        "    resume_checkpoint: str = None,\n",
        "    unet: UNet2DConditionModel = None,\n",
        "    accelerator: Accelerator = None,\n",
        "):\n",
        "    # 1) Prepare output directory, accelerator & device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    accel = accelerator or Accelerator()\n",
        "    device = accel.device\n",
        "\n",
        "    # 2) Load or resume UNet\n",
        "    if resume_checkpoint and unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(resume_checkpoint).to(device)\n",
        "    elif unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
        "\n",
        "    # 3) Load & freeze tokenizer + text encoder\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
        "    text_encoder.requires_grad_(False)\n",
        "\n",
        "    # 4) Load & freeze VAE\n",
        "    vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
        "    vae.requires_grad_(False)\n",
        "\n",
        "    # 5) Load noise scheduler\n",
        "    scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
        "\n",
        "    # 6) Prepare dataset & dataloader\n",
        "    dataset = ImgDataset(data_dir, tokenizer, cohort=cohort, resolution=resolution, dataset_size=dataset_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 7) Optimizer (only UNet params)\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 8) Wrap models, optimizer, and dataloader for mixed-precision / distributed\n",
        "    unet, optimizer, dataloader = accel.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # 9) Resume state if requested\n",
        "    if resume_checkpoint:\n",
        "        accel.load_state(resume_checkpoint)\n",
        "        global_step = int(resume_checkpoint.rsplit(\"_\", 1)[-1])\n",
        "    else:\n",
        "        global_step = 0\n",
        "\n",
        "    # 10) Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accel.accumulate(unet):\n",
        "                # Encode images to latents\n",
        "                pixels = batch[\"pixel_values\"].to(device)\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * 0.18215\n",
        "\n",
        "                # Add noise\n",
        "                noise = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
        "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Text conditioning\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "                # Noise prediction & loss\n",
        "                pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "\n",
        "                # Backpropagate\n",
        "                accel.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % save_steps == 0:\n",
        "                accel.wait_for_everyone()\n",
        "                ckpt_dir = os.path.join(output_dir, f\"checkpoint_{global_step}\")\n",
        "                unet.save_pretrained(ckpt_dir)\n",
        "                if accel.is_main_process:\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "                accel.save_state(ckpt_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} complete\")\n",
        "\n",
        "    # 11) Final save\n",
        "    accel.wait_for_everyone()\n",
        "    final_dir = os.path.join(output_dir, \"final_unet\")\n",
        "    unet.save_pretrained(final_dir)\n",
        "    if accel.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Fine-tuning complete — models saved to\", output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHUoyHIGVypU",
        "stage": {}
      },
      "source": [
        "### 3.4 Diffusion Model 학습 실행\n",
        "\n",
        "미리 학습된 모델(`nota-ai/bk-sdm-small`)을 사용하여 CNV 이미지로 파인튜닝을 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7am4VrDVVypU",
        "stage": {}
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# Accelerator 초기화\n",
        "accel = Accelerator()\n",
        "\n",
        "# Diffusion Model 학습 실행\n",
        "train(\n",
        "    pretrained_model=\"nota-ai/bk-sdm-small\",\n",
        "    data_dir=\"/content/processed/CNV\",\n",
        "    output_dir=\"/content/models/sd_cnv_finetuned\",\n",
        "    cohort=\"CNV\",\n",
        "    resolution=512,\n",
        "    dataset_size=500,  # 학습 시간 고려\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    epochs=10,\n",
        "    grad_accum_steps=1,\n",
        "    save_steps=1000,\n",
        "    resume_checkpoint=None,\n",
        "    accelerator=accel\n",
        ")\n",

System: It appears that the response was cut off due to length constraints. I'll continue from where the previous response left off, completing the `.ipynb` file with all remaining cells, ensuring each `"metadata"` object includes `"stage": {}`, and maintaining the original structure and content. The artifact will use the same `artifact_id` since this is an update to the previous response.

<xaiArtifact artifact_id="39630dc2-6a98-4646-9b57-6f6900c8eb6a" artifact_version_id="bbecaaa9-f4f0-417c-a4b7-645bc01123c7" title="1st_stage_synthetic_cnv_classifier.ipynb" contentType="application/x-ipynb+json">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "stage": {}
      },
      "source": [
        "<a href=\"https://colab.research.google.com/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title",
        "stage": {}
      },
      "source": [
        "# Synthetic CNV Image Classifier\n",
        "\n",
        "이 노트북은 OCT2017 데이터셋의 CNV 이미지를 사용하여 Stable Diffusion 모델로 Synthetic CNV 이미지를 생성하고, 이를 Normal CNV 이미지와 다양한 비율(0%~100%)로 혼합하여 Classifier를 학습시킵니다. 목표는 Synthetic 이미지의 최적 비율을 찾아 Classifier 성능을 최대화하는 것입니다.\n",
        "\n",
        "**실행 순서**:\n",
        "1. 데이터 전처리: OCT2017 CNV/NORMAL 이미지 → ViT 임베딩 → `pre-trained_dataset.pt`.\n",
        "2. Diffusion Model 파인튜닝: Stable Diffusion UNet을 CNV 이미지로 학습.\n",
        "3. Synthetic CNV 이미지 생성: 파인튜닝된 모델로 이미지 생성 → ViT 임베딩 → `synthetic_cnv_dataset.pt`.\n",
        "4. 비율별 Classifier 학습: Synthetic 비율 0%~100%로 데이터 혼합 → Classifier 학습 → 성능 비교.\n",
        "\n",
        "**실행 환경**: Google Colab (GPU, 예: T4 또는 A100 권장)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup",
        "stage": {}
      },
      "source": [
        "## 1. 환경 설정 및 패키지 설치\n",
        "\n",
        "필요한 Python 패키지를 설치하고, 랜덤 시드를 설정하여 재현성을 보장합니다. GPU를 사용하며, 메모리 관리를 위한 함수도 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63b91c-30f4-41a0-e533-602ba2b07e86",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "... (출력 내용은 동일하므로 생략)"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# 메모리 정리 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess",
        "stage": {}
      },
      "source": [
        "## 2. 데이터 전처리: OCT2017 데이터셋 → ViT 임베딩\n",
        "\n",
        "OCT2017 데이터셋의 CNV 및 NORMAL 이미지를 로드하고, ViT 모델을 사용하여 임베딩을 생성합니다. 결과는 `pre-trained_dataset.pt` 파일로 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5dc3a3-d8a6-4f2b-90d4-8f2531a22dbd",
        "stage": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing: Start!\n"
          ]
        }
      ],
      "source": [
        "def img_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Lambda(lambda img: img.crop((0, 100, 768, 400))),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def vit_process(img_list, batch_size=16):\n",
        "    model_name = \"google/vit-base-patch16-224\"\n",
        "    vit_model = ViTModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
        "    vit_model.eval()\n",
        "    length = len(img_list)\n",
        "    layers_0, layers_1, layers_2, layers_3 = [], [], [], []\n",
        "    for i in range(int(length/batch_size)):\n",
        "        batch_imgs = img_list[i*batch_size:(i+1)*batch_size]\n",
        "        batch_tensor = torch.stack ಬ