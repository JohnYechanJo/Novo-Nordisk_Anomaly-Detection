{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj8NclMRKoNYUImQ9BJNqW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnYechanJo/Novo-Nordisk_Anomaly-Detection/blob/initial-diffusion-model/diffusion_model_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uY5gjJnSEZd"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub torch torchvision transformers diffusers accelerate datasets xformers pytorch-fid pandas\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from transformers import ViTModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Memory cleanup function\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Diffusion Model Fine-tuning\n",
        "Fine-tune the UNet of the Stable Diffusion model using CNV images.\n",
        "Download the dataset from Kaggle, preprocess the CNV images, and use them for training.\n",
        "The trained model will be saved to `/content/models/sd_cnv_finetuned.`"
      ],
      "metadata": {
        "id": "egmv659mSto3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Data Preparation\n",
        "Download CNV images from the Kaggle dataset, resize them to 512x512, and save them to `/content/processed/CNV/.`"
      ],
      "metadata": {
        "id": "n7Ciq7lNSwAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_cnv_images():\n",
        "    # Kaggle dataset download\n",
        "    path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # Set dataset path and copy CNV images\n",
        "    in_dir = os.path.join(path, \"OCT2017 /train/CNV\")\n",
        "    out_dir = \"/content/processed/CNV/\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for fn in os.listdir(in_dir):\n",
        "        img = Image.open(os.path.join(in_dir, fn)).convert(\"RGB\")\n",
        "        img = img.resize((512, 512), resample=Image.LANCZOS)\n",
        "        img.save(os.path.join(out_dir, fn))\n",
        "    print(f\"Processed CNV images saved to {out_dir}\")\n",
        "\n",
        "prepare_cnv_images()"
      ],
      "metadata": {
        "id": "thikaIm5Sqhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Dataset Class\n",
        "Define the `ImgDataset` class to load CNV images and transform them into the format required for training, with detailed prompt."
      ],
      "metadata": {
        "id": "BG-ViYe2S34F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, root_dir:str, tokenizer, prompt:str = \"OCT scan showing CNV\", resolution=512, max_length=77, dataset_size:int = 6400) -> None:\n",
        "        self.files = [\n",
        "            os.path.join(root_dir, f)\n",
        "            for f in os.listdir(root_dir)\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ]\n",
        "        if len(self.files) > dataset_size: self.files = self.files[:dataset_size]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "        self.prompt = prompt\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> dict:\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze(0)}"
      ],
      "metadata": {
        "id": "8ODNMDmXS_S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Training Function\n",
        "Define the `train` function to fine-tune the Stable Diffusion UNet using CNV images. It supports mixed precision training and checkpoint saving."
      ],
      "metadata": {
        "id": "qDCDHRjvTJRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "def train(\n",
        "    pretrained_model: str,\n",
        "    data_dir: str,\n",
        "    output_dir: str = \"sd_cnv_finetuned\",\n",
        "    prompt:str = \"OCT scan showing CNV\",\n",
        "    resolution: int = 512,\n",
        "    dataset_size: int = 6400,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-4,\n",
        "    epochs: int = 5,\n",
        "    grad_accum_steps: int = 1,\n",
        "    save_steps: int = 1000,\n",
        "    resume_checkpoint: str = None,\n",
        "    unet: UNet2DConditionModel = None,\n",
        "    accelerator: Accelerator = None,\n",
        "):\n",
        "    # 1) Prepare output directory, accelerator & device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    accel = accelerator or Accelerator()\n",
        "    device = accel.device\n",
        "\n",
        "    # 2) Load or resume UNet\n",
        "    if resume_checkpoint and unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(resume_checkpoint).to(device)\n",
        "    elif unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
        "\n",
        "    # 3) Load & freeze tokenizer + text encoder\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
        "    text_encoder.requires_grad_(False)\n",
        "\n",
        "    # 4) Load & freeze VAE\n",
        "    vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
        "    vae.requires_grad_(False)\n",
        "\n",
        "    # 5) Load noise scheduler\n",
        "    scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
        "\n",
        "    # 6) Prepare dataset & dataloader\n",
        "    dataset = ImgDataset(data_dir, tokenizer, prompt = prompt, resolution=resolution, dataset_size=dataset_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 7) Optimizer (only UNet params)\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 8) Wrap models, optimizer, and dataloader for mixed-precision / distributed\n",
        "    unet, optimizer, dataloader = accel.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # 9) Resume state if requested\n",
        "    if resume_checkpoint:\n",
        "        accel.load_state(resume_checkpoint)\n",
        "        global_step = int(resume_checkpoint.rsplit(\"_\", 1)[-1])\n",
        "    else:\n",
        "        global_step = 0\n",
        "\n",
        "    # 10) Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accel.accumulate(unet):\n",
        "                # Encode images to latents\n",
        "                pixels = batch[\"pixel_values\"].to(device)\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * 0.18215\n",
        "\n",
        "                # Add noise\n",
        "                noise = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
        "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Text conditioning\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "                # Noise prediction & loss\n",
        "                pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "\n",
        "                # Backpropagate\n",
        "                accel.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % save_steps == 0:\n",
        "                accel.wait_for_everyone()\n",
        "                ckpt_dir = os.path.join(output_dir, f\"checkpoint_{global_step}\")\n",
        "                unet.save_pretrained(ckpt_dir)\n",
        "                if accel.is_main_process:\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "                accel.save_state(ckpt_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} complete\")\n",
        "\n",
        "    # 11) Final save\n",
        "    accel.wait_for_everyone()\n",
        "    final_dir = os.path.join(output_dir, \"final_unet\")\n",
        "    unet.save_pretrained(final_dir)\n",
        "    if accel.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Fine-tuning complete — models saved to\", output_dir)"
      ],
      "metadata": {
        "id": "C3-Dk7xYTQPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Execute Diffusion Model Training\n",
        "Use the pre-trained model (`nota-ai/bk-sdm-small`) to fine-tune with CNV images."
      ],
      "metadata": {
        "id": "6rDmkbS6TWZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# Accelerator initialization\n",
        "accel = Accelerator()\n",
        "\n",
        "# Change prompt and see what happens\n",
        "detailed_prompt = \"Optical Coherence Tomography Scacn of Choroidal neovascularization retina\"\n",
        "\n",
        "# Excute Diffusion Model training\n",
        "train(\n",
        "    pretrained_model=\"nota-ai/bk-sdm-small\",\n",
        "    data_dir=\"/content/processed/CNV\",\n",
        "    output_dir=\"/content/models/sd_cnv_finetuned\",\n",
        "    prompt = detailed_prompt,\n",
        "    resolution=512,\n",
        "    dataset_size=500,  # Consideration of Training Time\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    epochs=10,\n",
        "    grad_accum_steps=1,\n",
        "    save_steps=1000,\n",
        "    resume_checkpoint=None,\n",
        "    accelerator=accel\n",
        ")\n",
        "\n",
        "# Memory clean up\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "kElyAnPuTZ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Generate Synthetic CNV Images\n",
        "Use the fine-tuned Diffusion Model to generate synthetic CNV images."
      ],
      "metadata": {
        "id": "KAKsgLxJUIsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_images():\n",
        "    # Load the Base Pipeline from the Original Model\n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        \"nota-ai/bk-sdm-small\",\n",
        "        torch_dtype=torch.float16,\n",
        "        use_auth_token=False\n",
        "    ).to(device)\n",
        "\n",
        "    # Load the Fine-tuned UNet\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        \"/content/models/sd_cnv_finetuned/final_unet\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "\n",
        "    # Replace the UNet in the Pipeline\n",
        "    pipeline.unet = unet\n",
        "\n",
        "    # Generate Synthetic Images\n",
        "    synthetic_dir = \"/content/synthetic_cnv/\"\n",
        "    os.makedirs(synthetic_dir, exist_ok=True)\n",
        "    num_images = 640\n",
        "    prompt = (\n",
        "    f\"High-resolution grayscale {detailed_prompt}\"\n",
        "    \"Clear layer definition, with details of retinal features. Dark background, minimal color, realistic biomedical texture.\"\n",
        "    )\n",
        "\n",
        "\n",
        "    for i in range(num_images):\n",
        "        image = pipeline(prompt, num_inference_steps=50).images[0]\n",
        "        image.save(os.path.join(synthetic_dir, f\"synthetic_cnv_{i}.png\"))\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Generated {i}/{num_images} images\")\n",
        "        clear_memory()\n",
        "\n",
        "generate_synthetic_images()\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "Uwxuk8zUUYom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Example of Generated Images"
      ],
      "metadata": {
        "id": "ppMrNjV9VVpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to display images in a grid\n",
        "def display_images_grid(image_paths, num_cols=4):\n",
        "    num_images = len(image_paths)\n",
        "    num_rows = (num_images + num_cols - 1) // num_cols\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 3 * num_rows))\n",
        "    for i, path in enumerate(image_paths):\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "        try:\n",
        "            image = plt.imread(path)\n",
        "            axes[row, col].imshow(image)\n",
        "            axes[row, col].axis('off')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {path}\")\n",
        "        except Exception as e:\n",
        "          print(f\"Error loading image {path}: {e}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get a list of image file paths\n",
        "image_dir = \"/content/synthetic_cnv/\"\n",
        "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "\n",
        "# Randomly select 16 images\n",
        "random_images = random.sample(image_files, 16)\n",
        "\n",
        "# Display the selected images in a grid\n",
        "display_images_grid(random_images)\n"
      ],
      "metadata": {
        "id": "-BnGPE-UVb6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}