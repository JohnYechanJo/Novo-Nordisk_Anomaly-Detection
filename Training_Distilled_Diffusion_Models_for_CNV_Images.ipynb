{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrizdHigpDET"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iri4dCcCaXSv",
        "outputId": "3df54b22-47df-4126-aeee-eaa0eaf3da88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/kermany2018?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10.8G/10.8G [02:02<00:00, 95.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/paultimothymooney/kermany2018/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/kermany2018\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iYXj1DcYyFnd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"./data/\", exist_ok=True)\n",
        "\n",
        "for cohort in [\"CNV\", \"NORMAL\"]:\n",
        "\n",
        "    ! cp -r \"{path}/OCT2017 /train/{cohort}/\" ./data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J3GnWgrpHUK"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UVcaZex0kMWf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT-1-9HapAEf"
      },
      "outputs": [],
      "source": [
        "class ImgDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir:str, tokenizer, cohort:str = \"CNV\", resolution=512, max_length=77, dataset_size:int = 6400) -> None:\n",
        "\n",
        "        self.files = [\n",
        "            os.path.join(root_dir, f)\n",
        "            for f in os.listdir(root_dir)\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ]\n",
        "\n",
        "        if len(self.files) > dataset_size: self.files = self.files[:dataset_size]\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((resolution, resolution), transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "        ])\n",
        "\n",
        "        self.prompt = f\"OCT scan showing {cohort}\"\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx:int) -> dict:\n",
        "\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze(0)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FhhBRaUorAg-"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    pretrained_model: str,\n",
        "    data_dir: str,\n",
        "    output_dir: str = \"sd_cnv_finetuned\",\n",
        "    cohort: str = \"CNV\",\n",
        "    resolution: int = 512,\n",
        "    dataset_size:int = 6400,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-4,\n",
        "    epochs: int = 5,\n",
        "    grad_accum_steps: int = 1,\n",
        "    save_steps: int = 1000,\n",
        "    resume_checkpoint: str = None,\n",
        "    unet: UNet2DConditionModel = None,\n",
        "    accelerator: Accelerator = None,\n",
        "):\n",
        "\n",
        "    # 1) Prepare output directory, accelerator & device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    accel = accelerator or Accelerator()\n",
        "    device = accel.device\n",
        "\n",
        "    # 2) Load or resume UNet\n",
        "    if resume_checkpoint and unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(resume_checkpoint).to(device)\n",
        "    elif unet is None:\n",
        "        unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
        "\n",
        "    # 3) Load & freeze tokenizer + text encoder\n",
        "    tokenizer    = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
        "    text_encoder.requires_grad_(False)\n",
        "\n",
        "    # 4) Load & freeze VAE\n",
        "    vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
        "    vae.requires_grad_(False)\n",
        "\n",
        "    # 5) Load noise scheduler\n",
        "    scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
        "\n",
        "    # 6) Prepare dataset & dataloader (assumes CNVDataset is defined)\n",
        "    dataset    = ImgDataset(data_dir, tokenizer, cohort = cohort, resolution=resolution, dataset_size = dataset_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 7) Optimizer (only UNet params)\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 8) Wrap models, optimizer, and dataloader for mixed‑precision / distributed\n",
        "    unet, optimizer, dataloader = accel.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # 9) Resume state if requested\n",
        "    if resume_checkpoint:\n",
        "        accel.load_state(resume_checkpoint)\n",
        "        # pick up where you left off\n",
        "        global_step = int(resume_checkpoint.rsplit(\"_\", 1)[-1])\n",
        "    else:\n",
        "        global_step = 0\n",
        "\n",
        "    # 10) Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accel.accumulate(unet):\n",
        "                # Encode images to latents\n",
        "                pixels  = batch[\"pixel_values\"].to(device)\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * 0.18215\n",
        "\n",
        "                # Add noise\n",
        "                noise     = torch.randn_like(latents)\n",
        "                timesteps = torch.randint(0, scheduler.num_train_timesteps,\n",
        "                                          (latents.shape[0],), device=device)\n",
        "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Text conditioning\n",
        "                input_ids             = batch[\"input_ids\"].to(device)\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "                # Noise prediction & loss\n",
        "                pred_noise = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss       = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "\n",
        "                # Backpropagate\n",
        "                accel.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % save_steps == 0:\n",
        "                accel.wait_for_everyone()\n",
        "                ckpt_dir = os.path.join(output_dir, f\"checkpoint_{global_step}\")\n",
        "                unet.save_pretrained(ckpt_dir)\n",
        "                if accel.is_main_process:\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "                accel.save_state(ckpt_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} complete\")\n",
        "\n",
        "    # 11) Final save\n",
        "    accel.wait_for_everyone()\n",
        "    final_dir = os.path.join(output_dir, \"final_unet\")\n",
        "    unet.save_pretrained(final_dir)\n",
        "    if accel.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Fine-tuning complete — models saved to\", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICIL8dmxrVIU",
        "outputId": "68166029-b1e2-4094-8b3a-e634764cd1c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 complete\n",
            "Epoch 2/10 complete\n",
            "Epoch 3/10 complete\n",
            "Epoch 4/10 complete\n",
            "Epoch 5/10 complete\n",
            "Epoch 6/10 complete\n",
            "Epoch 7/10 complete\n",
            "Epoch 8/10 complete\n",
            "Epoch 9/10 complete\n",
            "Epoch 10/10 complete\n",
            "Fine-tuning complete — models saved to ./models/sd_cnv_finetuned\n"
          ]
        }
      ],
      "source": [
        "from accelerate import Accelerator\n",
        "# Optionally load the checkpoint’s state:\n",
        "# from https://huggingface.co/nota-ai/bk-sdm-small\n",
        "accel = Accelerator()\n",
        "train(\n",
        "    pretrained_model=\"nota-ai/bk-sdm-small\",\n",
        "    data_dir=\"./data/CNV\",\n",
        "    output_dir=\"./models/sd_cnv_finetuned\",\n",
        "    cohort = \"CNV\",\n",
        "    resolution=512,\n",
        "    dataset_size = 500, # 6 min per 100 data\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    epochs=10,\n",
        "    grad_accum_steps=1,\n",
        "    save_steps=1000,\n",
        "    resume_checkpoint=None,\n",
        "    accelerator=accel\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt7k2Q3yJChL",
        "outputId": "fa7f7f0c-b029-4548-bd70-09dd34e62206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: models/ (stored 0%)\n",
            "  adding: models/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: models/sd_cnv_finetuned/ (stored 0%)\n",
            "  adding: models/sd_cnv_finetuned/final_unet/ (stored 0%)\n",
            "  adding: models/sd_cnv_finetuned/final_unet/config.json (deflated 66%)\n",
            "  adding: models/sd_cnv_finetuned/final_unet/diffusion_pytorch_model.safetensors (deflated 7%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/ (stored 0%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/random_states_0.pkl (deflated 25%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/optimizer.bin (deflated 9%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/model.safetensors (deflated 7%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/config.json (deflated 66%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/special_tokens_map.json (deflated 73%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/diffusion_pytorch_model.safetensors (deflated 7%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/tokenizer_config.json (deflated 63%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/merges.txt (deflated 60%)\n",
            "  adding: models/sd_cnv_finetuned/checkpoint_1000/vocab.json (deflated 71%)\n",
            "  adding: models/sd_cnv_finetuned/special_tokens_map.json (deflated 73%)\n",
            "  adding: models/sd_cnv_finetuned/tokenizer_config.json (deflated 63%)\n",
            "  adding: models/sd_cnv_finetuned/merges.txt (deflated 60%)\n",
            "  adding: models/sd_cnv_finetuned/vocab.json (deflated 71%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r models.zip ./models/\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
